{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a932bfb-78a5-44d8-8804-fe355e5452bf",
      "metadata": {
        "id": "3a932bfb-78a5-44d8-8804-fe355e5452bf"
      },
      "outputs": [],
      "source": [
        "# General libraries\n",
        "import json\n",
        "from pathlib import Path as Data_Path\n",
        "import os\n",
        "from os.path import isfile, join\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f43ccfc-d0a5-4cbb-895b-7ad759a68c0c",
      "metadata": {
        "id": "2f43ccfc-d0a5-4cbb-895b-7ad759a68c0c",
        "outputId": "db1531f2-8430-469c-fb54-745dc167da99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Torch version: 2.5.1+cpu; Torch-cuda version: None; Torch Geometric version: 2.6.1.\n"
          ]
        }
      ],
      "source": [
        "# Import relevant ML libraries\n",
        "from typing import Optional, Union\n",
        "\n",
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.nn as nn\n",
        "from torch.nn import Embedding, ModuleList, Linear\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch_geometric\n",
        "import torch_geometric.nn as pyg_nn\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.transforms import RandomLinkSplit\n",
        "from torch.nn.modules.loss import _Loss\n",
        "\n",
        "from torch_geometric.nn.conv import LGConv, GATConv, SAGEConv\n",
        "from torch_geometric.typing import Adj, OptTensor, SparseTensor\n",
        "\n",
        "print(f\"Torch version: {torch.__version__}; Torch-cuda version: {torch.version.cuda}; Torch Geometric version: {torch_geometric.__version__}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2dc76bd3-ac2b-4c5c-ad91-d6fef0f84759",
      "metadata": {
        "id": "2dc76bd3-ac2b-4c5c-ad91-d6fef0f84759"
      },
      "outputs": [],
      "source": [
        "# Define the main directory\n",
        "MAIN_DIR = r\"D:\\CPEN355\"  # Use raw string to handle backslashes correctly\n",
        "\n",
        "# Define the data directory\n",
        "DATA_DIR = r\"D:\\CPEN355\\HalfMil\\FirstHalfData\"  # Correct the path and use a raw string\n",
        "\n",
        "# Change the current working directory to MAIN_DIR\n",
        "os.chdir(MAIN_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72d0c9c9-182f-400b-987f-5c68ae44c4d5",
      "metadata": {
        "id": "72d0c9c9-182f-400b-987f-5c68ae44c4d5"
      },
      "outputs": [],
      "source": [
        "# Citation reminder\n",
        "# TODO: create a method that returns the node info for networkx node generation\n",
        "import networkx as nx\n",
        "\n",
        "class Artist:\n",
        "  \"\"\"\n",
        "  Class for artist, containing attributes:\n",
        "    1. Artist_name\n",
        "    2. Artist_URI\n",
        "  \"\"\"\n",
        "  def __init__(self, track_dict):\n",
        "    self.artist_name = track_dict[\"artist_name\"]\n",
        "    self.artist_uri = track_dict[\"artist_uri\"]\n",
        "    # Maybe we should include albums too...\n",
        "\n",
        "  def __str__(self):\n",
        "    return f\"Artist {self.artist_name} with URI {self.artist_uri}\"\n",
        "\n",
        "  def node_info(self):\n",
        "    return {\"artist_name\": self.artist_name, \"node_type\": \"artist\"}\n",
        "\n",
        "class Album:\n",
        "  \"\"\"\n",
        "  Class for album, containing attributes:\n",
        "    1. Album_name\n",
        "    2. Album_URI\n",
        "  \"\"\"\n",
        "  def __init__(self, track_dict):\n",
        "    self.album_name = track_dict[\"album_name\"]\n",
        "    self.album_uri = track_dict[\"album_uri\"]\n",
        "\n",
        "  def __str__(self):\n",
        "    return f\"Album {self.name} with URI {self.uri}\"\n",
        "\n",
        "  def node_info(self):\n",
        "    return {\"album_name\": self.album_name, \"node_type\": \"album\"}\n",
        "\n",
        "class Track:\n",
        "  \"\"\"\n",
        "  Class for a track, containing its attributes:\n",
        "    1. URI (a unique id)\n",
        "    2. Name\n",
        "    3. Artist info (URI and name)\n",
        "    4. Parent playlist\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, track_dict, playlist):\n",
        "    self.uri = track_dict[\"track_uri\"]\n",
        "    self.name = track_dict[\"track_name\"]\n",
        "    self.duration_ms = track_dict[\"duration_ms\"]\n",
        "    self.album = Album(track_dict)\n",
        "    self.artist = Artist(track_dict)\n",
        "    self.playlist = playlist\n",
        "\n",
        "  def __str__(self):\n",
        "    return f\"Track {self.uri} called {self.name} by {self.artist.artist_name} ({self.artist.artist_name}) in playlist {self.playlist}.\"\n",
        "\n",
        "  def __repr__(self):\n",
        "    return f\"Track {self.uri}\"\n",
        "\n",
        "  def node_info(self):\n",
        "    return {\"track_name\": self.name, \"duration_ms\": self.duration_ms, \"node_type\": \"track\"}\n",
        "\n",
        "class Playlist:\n",
        "  \"\"\"\n",
        "  Class for a playlist, containing its attributes:\n",
        "    1. Name (playlist and its associated index)\n",
        "    2. Title (playlist title in the Spotify dataset)\n",
        "    3. Loaded dictionary from the raw json for the playlist\n",
        "    4. Dictionary of tracks (track_uri : Track), populated by .load_tracks()\n",
        "    5. List of artists uris\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, json_data, index):\n",
        "    self.name = f\"playlist_{index}\"\n",
        "    self.title = json_data[\"name\"]\n",
        "    self.data = json_data\n",
        "    self.num_albums = json_data[\"num_albums\"]\n",
        "    self.tracks = {}\n",
        "\n",
        "# Loads all of tracks in json data for playlist\n",
        "  def load_tracks(self):\n",
        "    tracks_list = self.data[\"tracks\"]\n",
        "    self.tracks = {x[\"track_uri\"] : Track(x, self.name) for x in tracks_list}\n",
        "\n",
        "  def __str__(self):\n",
        "    return f\"Playlist {self.name} with {len(self.tracks)} tracks loaded.\"\n",
        "\n",
        "  def __repr__(self):\n",
        "    return f\"Playlist {self.name}\"\n",
        "\n",
        "  def node_info(self):\n",
        "    node_info = {x: self.data[x] for x in self.data if x in [\"name\", \"num_tracks\", \"num_albums\", \"duration_ms\", \"num_artists\"]}\n",
        "    node_info[\"node_type\"] = \"playlist\"\n",
        "    return node_info\n",
        "\n",
        "\n",
        "class JSONFile:\n",
        "  \"\"\"\n",
        "  Class for a JSON file, containing its attributes:\n",
        "    1. File Name\n",
        "    2. Index to begin numbering playlists at\n",
        "    3. Loaded dictionary from the raw json for the full file\n",
        "    4. Dictionary of playlists (name : Playlist), populated by .process_file()\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, data_path, file_name, start_index):\n",
        "\n",
        "    self.file_name = file_name\n",
        "    self.start_index = start_index\n",
        "\n",
        "    with open(os.path.join(data_path, file_name)) as json_file:\n",
        "      json_data = json.load(json_file)\n",
        "    self.data = json_data\n",
        "\n",
        "    self.playlists = {}\n",
        "\n",
        "# Load all of playlists in json data\n",
        "  def process_file(self):\n",
        "    for i, playlist_json in enumerate(self.data[\"playlists\"]):\n",
        "      playlist = Playlist(playlist_json, self.start_index + i)\n",
        "      playlist.load_tracks()\n",
        "      self.playlists[playlist.name] = playlist\n",
        "\n",
        "  def __str__(self):\n",
        "    return f\"JSON {self.file_name} has {len(self.playlists)} playlists loaded.\"\n",
        "\n",
        "  def __repr__(self):\n",
        "    return self.file_name\n",
        "\n",
        "\n",
        "class DataSet:\n",
        "  \"\"\"\n",
        "  Class for the Spotify data, containing its attributes:\n",
        "    1. JSON File List\n",
        "    2. Dictionary of JSON files (file_name : JSONFile), populated by .process_files()\n",
        "    3. Number of files to keep\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, data_path, num_files_to_keep):\n",
        "    self.data_path = data_path\n",
        "    self.num_files_to_keep = num_files_to_keep\n",
        "    self.json_files = self.retrieve_files()\n",
        "    self.files = {}\n",
        "\n",
        "# Retrieve all json files from the dataset\n",
        "  def retrieve_files(self):\n",
        "    json_files = sorted(\n",
        "      [file for file in os.listdir(self.data_path) if file.endswith('.json')],\n",
        "      key=lambda f: int(''.join(filter(str.isdigit, f))) if any(char.isdigit() for char in f) else float('inf')\n",
        "    )[:self.num_files_to_keep]\n",
        "    return json_files\n",
        "\n",
        "# Load all of the files in dataset\n",
        "  def process_files(self):\n",
        "    for i, file in enumerate(self.json_files):\n",
        "      json_file = JSONFile(self.data_path, file, i*1000)\n",
        "      json_file.process_file()\n",
        "      self.files[json_file.file_name] = json_file\n",
        "      print(f\"Reading: {file}\")\n",
        "\n",
        "  def __str__(self):\n",
        "    return f\"DataSet with {len(self.files)} JSON files loaded.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4117f13-3289-4ad9-b01f-62d1c28ba4cd",
      "metadata": {
        "id": "c4117f13-3289-4ad9-b01f-62d1c28ba4cd"
      },
      "outputs": [],
      "source": [
        "# Modified NX Graph Generation\n",
        "\n",
        "import networkx as nx\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "class NXPlaylistGraph:\n",
        "  \"\"\"\n",
        "  Class for a playlist graph, containing its attributes:\n",
        "    1. NX Graph of playlists, songs, artists and albums, populated by .generate_graph()\n",
        "    2. Number of files to keep\n",
        "    3. Data Set\n",
        "    4. kcore to be used in nx.k_core\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, data_dir, num_files_to_keep, kcore):\n",
        "    self.num_files_to_keep = num_files_to_keep\n",
        "    self.kcore = kcore\n",
        "    self.data_set = DataSet(data_dir, self.num_files_to_keep)\n",
        "    self.data_set.process_files()\n",
        "    self.nx_graph = nx.Graph()\n",
        "\n",
        "# Generate a networkx graph from the playlist data\n",
        "  def generate_graph(self):\n",
        "    for file_name, json_file in self.data_set.files.items():\n",
        "      print(f\"Processing file: {file_name}\")\n",
        "\n",
        "      for playlist_name, playlist in json_file.playlists.items():\n",
        "        self.nx_graph.add_node(playlist_name, type=\"playlist\", name=playlist.title)\n",
        "\n",
        "\n",
        "        for track_uri, track in playlist.tracks.items():\n",
        "          self.nx_graph.add_node(track.uri, type=\"track\", name=track.name)\n",
        "          self.nx_graph.add_edge(track_uri, playlist_name, edge_type=\"track_to_playlist\")\n",
        "\n",
        "          # self.nx_graph.add_node(track.album.album_uri, type=\"album\", name=track.album.album_name)\n",
        "          # self.nx_graph.add_edge(track.album.album_uri, track_uri, edge_type=\"album_to_track\")\n",
        "          # self.nx_graph.add_edge(track.album.album_uri, playlist_name, edge_type=\"album_to_playlist\")\n",
        "\n",
        "          # self.nx_graph.add_node(track.artist.artist_uri, type=\"artist\", name=track.artist.artist_name)\n",
        "          # self.nx_graph.add_edge(track.artist.artist_uri, track_uri, edge_type=\"artist_to_track\") # Might not be necessary\n",
        "          # self.nx_graph.add_edge(playlist_name, track.artist.artist_uri, edge_type=\"playlist_to_artist\")\n",
        "          # self.nx_graph.add_edge(track.artist.artist_uri, track.album.album_uri, edge_type=\"artist_to_album\")\n",
        "\n",
        "    print(self.nx_graph)\n",
        "\n",
        "    self.nx_graph = nx.k_core(self.nx_graph, k=self.kcore)\n",
        "\n",
        "    print(self.nx_graph)\n",
        "\n",
        "  def get_graph(self):\n",
        "    return self.nx_graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "697cf90a-0bd0-4233-9bd3-170499d015cf",
      "metadata": {
        "id": "697cf90a-0bd0-4233-9bd3-170499d015cf",
        "outputId": "3ffad9ac-dfd2-4e49-9eeb-c5f8cea19e91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading: mpd.slice.0-999.json\n",
            "Reading: mpd.slice.1000-1999.json\n",
            "Reading: mpd.slice.2000-2999.json\n",
            "Reading: mpd.slice.3000-3999.json\n",
            "Reading: mpd.slice.4000-4999.json\n",
            "Reading: mpd.slice.5000-5999.json\n",
            "Reading: mpd.slice.6000-6999.json\n",
            "Reading: mpd.slice.7000-7999.json\n",
            "Reading: mpd.slice.8000-8999.json\n",
            "Reading: mpd.slice.9000-9999.json\n",
            "Reading: mpd.slice.10000-10999.json\n",
            "Reading: mpd.slice.11000-11999.json\n",
            "Reading: mpd.slice.12000-12999.json\n",
            "Reading: mpd.slice.13000-13999.json\n",
            "Reading: mpd.slice.14000-14999.json\n",
            "Reading: mpd.slice.15000-15999.json\n",
            "Reading: mpd.slice.16000-16999.json\n",
            "Reading: mpd.slice.17000-17999.json\n",
            "Reading: mpd.slice.18000-18999.json\n",
            "Reading: mpd.slice.19000-19999.json\n",
            "Reading: mpd.slice.20000-20999.json\n",
            "Reading: mpd.slice.21000-21999.json\n",
            "Reading: mpd.slice.22000-22999.json\n",
            "Reading: mpd.slice.23000-23999.json\n",
            "Reading: mpd.slice.24000-24999.json\n",
            "Reading: mpd.slice.25000-25999.json\n",
            "Reading: mpd.slice.26000-26999.json\n",
            "Reading: mpd.slice.27000-27999.json\n",
            "Reading: mpd.slice.28000-28999.json\n",
            "Reading: mpd.slice.29000-29999.json\n",
            "Reading: mpd.slice.30000-30999.json\n",
            "Reading: mpd.slice.31000-31999.json\n",
            "Reading: mpd.slice.32000-32999.json\n",
            "Reading: mpd.slice.33000-33999.json\n",
            "Reading: mpd.slice.34000-34999.json\n",
            "Reading: mpd.slice.35000-35999.json\n",
            "Reading: mpd.slice.36000-36999.json\n",
            "Reading: mpd.slice.37000-37999.json\n",
            "Reading: mpd.slice.38000-38999.json\n",
            "Reading: mpd.slice.39000-39999.json\n",
            "Reading: mpd.slice.40000-40999.json\n",
            "Reading: mpd.slice.41000-41999.json\n",
            "Reading: mpd.slice.42000-42999.json\n",
            "Reading: mpd.slice.43000-43999.json\n",
            "Reading: mpd.slice.44000-44999.json\n",
            "Reading: mpd.slice.45000-45999.json\n",
            "Reading: mpd.slice.46000-46999.json\n",
            "Reading: mpd.slice.47000-47999.json\n",
            "Reading: mpd.slice.48000-48999.json\n",
            "Reading: mpd.slice.49000-49999.json\n",
            "Reading: mpd.slice.50000-50999.json\n",
            "Reading: mpd.slice.51000-51999.json\n",
            "Reading: mpd.slice.52000-52999.json\n",
            "Reading: mpd.slice.53000-53999.json\n",
            "Reading: mpd.slice.54000-54999.json\n",
            "Reading: mpd.slice.55000-55999.json\n",
            "Reading: mpd.slice.56000-56999.json\n",
            "Reading: mpd.slice.57000-57999.json\n",
            "Reading: mpd.slice.58000-58999.json\n",
            "Reading: mpd.slice.59000-59999.json\n",
            "Reading: mpd.slice.60000-60999.json\n",
            "Reading: mpd.slice.61000-61999.json\n",
            "Reading: mpd.slice.62000-62999.json\n",
            "Reading: mpd.slice.63000-63999.json\n",
            "Reading: mpd.slice.64000-64999.json\n",
            "Reading: mpd.slice.65000-65999.json\n",
            "Reading: mpd.slice.66000-66999.json\n",
            "Reading: mpd.slice.67000-67999.json\n",
            "Reading: mpd.slice.68000-68999.json\n",
            "Reading: mpd.slice.69000-69999.json\n",
            "Reading: mpd.slice.70000-70999.json\n",
            "Reading: mpd.slice.71000-71999.json\n",
            "Reading: mpd.slice.72000-72999.json\n",
            "Reading: mpd.slice.73000-73999.json\n",
            "Reading: mpd.slice.74000-74999.json\n",
            "Reading: mpd.slice.75000-75999.json\n",
            "Reading: mpd.slice.76000-76999.json\n",
            "Reading: mpd.slice.77000-77999.json\n",
            "Reading: mpd.slice.78000-78999.json\n",
            "Reading: mpd.slice.79000-79999.json\n",
            "Reading: mpd.slice.80000-80999.json\n",
            "Reading: mpd.slice.81000-81999.json\n",
            "Reading: mpd.slice.82000-82999.json\n",
            "Reading: mpd.slice.83000-83999.json\n",
            "Reading: mpd.slice.84000-84999.json\n",
            "Reading: mpd.slice.85000-85999.json\n",
            "Reading: mpd.slice.86000-86999.json\n",
            "Reading: mpd.slice.87000-87999.json\n",
            "Reading: mpd.slice.88000-88999.json\n",
            "Reading: mpd.slice.89000-89999.json\n",
            "Reading: mpd.slice.90000-90999.json\n",
            "Reading: mpd.slice.91000-91999.json\n",
            "Reading: mpd.slice.92000-92999.json\n",
            "Reading: mpd.slice.93000-93999.json\n",
            "Reading: mpd.slice.94000-94999.json\n",
            "Reading: mpd.slice.95000-95999.json\n",
            "Reading: mpd.slice.96000-96999.json\n",
            "Reading: mpd.slice.97000-97999.json\n",
            "Reading: mpd.slice.98000-98999.json\n",
            "Reading: mpd.slice.99000-99999.json\n",
            "Reading: mpd.slice.100000-100999.json\n",
            "Reading: mpd.slice.101000-101999.json\n",
            "Reading: mpd.slice.102000-102999.json\n",
            "Reading: mpd.slice.103000-103999.json\n",
            "Reading: mpd.slice.104000-104999.json\n",
            "Reading: mpd.slice.105000-105999.json\n",
            "Reading: mpd.slice.106000-106999.json\n",
            "Reading: mpd.slice.107000-107999.json\n",
            "Reading: mpd.slice.108000-108999.json\n",
            "Reading: mpd.slice.109000-109999.json\n",
            "Reading: mpd.slice.110000-110999.json\n",
            "Reading: mpd.slice.111000-111999.json\n",
            "Reading: mpd.slice.112000-112999.json\n",
            "Reading: mpd.slice.113000-113999.json\n",
            "Reading: mpd.slice.114000-114999.json\n",
            "Reading: mpd.slice.115000-115999.json\n",
            "Reading: mpd.slice.116000-116999.json\n",
            "Reading: mpd.slice.117000-117999.json\n",
            "Reading: mpd.slice.118000-118999.json\n",
            "Reading: mpd.slice.119000-119999.json\n",
            "Reading: mpd.slice.120000-120999.json\n",
            "Reading: mpd.slice.121000-121999.json\n",
            "Reading: mpd.slice.122000-122999.json\n",
            "Reading: mpd.slice.123000-123999.json\n",
            "Reading: mpd.slice.124000-124999.json\n",
            "Reading: mpd.slice.125000-125999.json\n",
            "Reading: mpd.slice.126000-126999.json\n",
            "Reading: mpd.slice.127000-127999.json\n",
            "Reading: mpd.slice.128000-128999.json\n",
            "Reading: mpd.slice.129000-129999.json\n",
            "Reading: mpd.slice.130000-130999.json\n",
            "Reading: mpd.slice.131000-131999.json\n",
            "Reading: mpd.slice.132000-132999.json\n",
            "Reading: mpd.slice.133000-133999.json\n",
            "Reading: mpd.slice.134000-134999.json\n",
            "Reading: mpd.slice.135000-135999.json\n",
            "Reading: mpd.slice.136000-136999.json\n",
            "Reading: mpd.slice.137000-137999.json\n",
            "Reading: mpd.slice.138000-138999.json\n",
            "Reading: mpd.slice.139000-139999.json\n",
            "Reading: mpd.slice.140000-140999.json\n",
            "Reading: mpd.slice.141000-141999.json\n",
            "Reading: mpd.slice.142000-142999.json\n",
            "Reading: mpd.slice.143000-143999.json\n",
            "Reading: mpd.slice.144000-144999.json\n",
            "Reading: mpd.slice.145000-145999.json\n",
            "Reading: mpd.slice.146000-146999.json\n",
            "Reading: mpd.slice.147000-147999.json\n",
            "Reading: mpd.slice.148000-148999.json\n",
            "Reading: mpd.slice.149000-149999.json\n",
            "Reading: mpd.slice.150000-150999.json\n",
            "Reading: mpd.slice.151000-151999.json\n",
            "Reading: mpd.slice.152000-152999.json\n",
            "Reading: mpd.slice.153000-153999.json\n",
            "Reading: mpd.slice.154000-154999.json\n",
            "Reading: mpd.slice.155000-155999.json\n",
            "Reading: mpd.slice.156000-156999.json\n",
            "Reading: mpd.slice.157000-157999.json\n",
            "Reading: mpd.slice.158000-158999.json\n",
            "Reading: mpd.slice.159000-159999.json\n",
            "Reading: mpd.slice.160000-160999.json\n",
            "Reading: mpd.slice.161000-161999.json\n",
            "Reading: mpd.slice.162000-162999.json\n",
            "Reading: mpd.slice.163000-163999.json\n",
            "Reading: mpd.slice.164000-164999.json\n",
            "Reading: mpd.slice.165000-165999.json\n",
            "Reading: mpd.slice.166000-166999.json\n",
            "Reading: mpd.slice.167000-167999.json\n",
            "Reading: mpd.slice.168000-168999.json\n",
            "Reading: mpd.slice.169000-169999.json\n",
            "Reading: mpd.slice.170000-170999.json\n",
            "Reading: mpd.slice.171000-171999.json\n",
            "Reading: mpd.slice.172000-172999.json\n",
            "Reading: mpd.slice.173000-173999.json\n",
            "Reading: mpd.slice.174000-174999.json\n",
            "Reading: mpd.slice.175000-175999.json\n",
            "Reading: mpd.slice.176000-176999.json\n",
            "Reading: mpd.slice.177000-177999.json\n",
            "Reading: mpd.slice.178000-178999.json\n",
            "Reading: mpd.slice.179000-179999.json\n",
            "Reading: mpd.slice.180000-180999.json\n",
            "Reading: mpd.slice.181000-181999.json\n",
            "Reading: mpd.slice.182000-182999.json\n",
            "Reading: mpd.slice.183000-183999.json\n",
            "Reading: mpd.slice.184000-184999.json\n",
            "Reading: mpd.slice.185000-185999.json\n",
            "Reading: mpd.slice.186000-186999.json\n",
            "Reading: mpd.slice.187000-187999.json\n",
            "Reading: mpd.slice.188000-188999.json\n",
            "Reading: mpd.slice.189000-189999.json\n",
            "Reading: mpd.slice.190000-190999.json\n",
            "Reading: mpd.slice.191000-191999.json\n",
            "Reading: mpd.slice.192000-192999.json\n",
            "Reading: mpd.slice.193000-193999.json\n",
            "Reading: mpd.slice.194000-194999.json\n",
            "Reading: mpd.slice.195000-195999.json\n",
            "Reading: mpd.slice.196000-196999.json\n",
            "Reading: mpd.slice.197000-197999.json\n",
            "Reading: mpd.slice.198000-198999.json\n",
            "Reading: mpd.slice.199000-199999.json\n",
            "Reading: mpd.slice.200000-200999.json\n",
            "Reading: mpd.slice.201000-201999.json\n",
            "Reading: mpd.slice.202000-202999.json\n",
            "Reading: mpd.slice.203000-203999.json\n",
            "Reading: mpd.slice.204000-204999.json\n",
            "Reading: mpd.slice.205000-205999.json\n",
            "Reading: mpd.slice.206000-206999.json\n",
            "Reading: mpd.slice.207000-207999.json\n",
            "Reading: mpd.slice.208000-208999.json\n",
            "Reading: mpd.slice.209000-209999.json\n",
            "Reading: mpd.slice.210000-210999.json\n",
            "Reading: mpd.slice.211000-211999.json\n",
            "Reading: mpd.slice.212000-212999.json\n",
            "Reading: mpd.slice.213000-213999.json\n",
            "Reading: mpd.slice.214000-214999.json\n",
            "Reading: mpd.slice.215000-215999.json\n",
            "Reading: mpd.slice.216000-216999.json\n",
            "Reading: mpd.slice.217000-217999.json\n",
            "Reading: mpd.slice.218000-218999.json\n",
            "Reading: mpd.slice.219000-219999.json\n",
            "Reading: mpd.slice.220000-220999.json\n",
            "Reading: mpd.slice.221000-221999.json\n",
            "Reading: mpd.slice.222000-222999.json\n",
            "Reading: mpd.slice.223000-223999.json\n",
            "Reading: mpd.slice.224000-224999.json\n",
            "Reading: mpd.slice.225000-225999.json\n",
            "Reading: mpd.slice.226000-226999.json\n",
            "Reading: mpd.slice.227000-227999.json\n",
            "Reading: mpd.slice.228000-228999.json\n",
            "Reading: mpd.slice.229000-229999.json\n",
            "Reading: mpd.slice.230000-230999.json\n",
            "Reading: mpd.slice.231000-231999.json\n",
            "Reading: mpd.slice.232000-232999.json\n",
            "Reading: mpd.slice.233000-233999.json\n",
            "Reading: mpd.slice.234000-234999.json\n",
            "Reading: mpd.slice.235000-235999.json\n",
            "Reading: mpd.slice.236000-236999.json\n",
            "Reading: mpd.slice.237000-237999.json\n",
            "Reading: mpd.slice.238000-238999.json\n",
            "Reading: mpd.slice.239000-239999.json\n",
            "Reading: mpd.slice.240000-240999.json\n",
            "Reading: mpd.slice.241000-241999.json\n",
            "Reading: mpd.slice.242000-242999.json\n",
            "Reading: mpd.slice.243000-243999.json\n",
            "Reading: mpd.slice.244000-244999.json\n",
            "Reading: mpd.slice.245000-245999.json\n",
            "Reading: mpd.slice.246000-246999.json\n",
            "Reading: mpd.slice.247000-247999.json\n",
            "Reading: mpd.slice.248000-248999.json\n",
            "Reading: mpd.slice.249000-249999.json\n",
            "Reading: mpd.slice.250000-250999.json\n",
            "Reading: mpd.slice.251000-251999.json\n",
            "Reading: mpd.slice.252000-252999.json\n",
            "Reading: mpd.slice.253000-253999.json\n",
            "Reading: mpd.slice.254000-254999.json\n",
            "Reading: mpd.slice.255000-255999.json\n",
            "Reading: mpd.slice.256000-256999.json\n",
            "Reading: mpd.slice.257000-257999.json\n",
            "Reading: mpd.slice.258000-258999.json\n",
            "Reading: mpd.slice.259000-259999.json\n",
            "Reading: mpd.slice.260000-260999.json\n",
            "Reading: mpd.slice.261000-261999.json\n",
            "Reading: mpd.slice.262000-262999.json\n",
            "Reading: mpd.slice.263000-263999.json\n",
            "Reading: mpd.slice.264000-264999.json\n",
            "Reading: mpd.slice.265000-265999.json\n",
            "Reading: mpd.slice.266000-266999.json\n",
            "Reading: mpd.slice.267000-267999.json\n",
            "Reading: mpd.slice.268000-268999.json\n",
            "Reading: mpd.slice.269000-269999.json\n",
            "Reading: mpd.slice.270000-270999.json\n",
            "Reading: mpd.slice.271000-271999.json\n",
            "Reading: mpd.slice.272000-272999.json\n",
            "Reading: mpd.slice.273000-273999.json\n",
            "Reading: mpd.slice.274000-274999.json\n",
            "Reading: mpd.slice.275000-275999.json\n",
            "Reading: mpd.slice.276000-276999.json\n",
            "Reading: mpd.slice.277000-277999.json\n",
            "Reading: mpd.slice.278000-278999.json\n",
            "Reading: mpd.slice.279000-279999.json\n",
            "Reading: mpd.slice.280000-280999.json\n",
            "Reading: mpd.slice.281000-281999.json\n",
            "Reading: mpd.slice.282000-282999.json\n",
            "Reading: mpd.slice.283000-283999.json\n",
            "Reading: mpd.slice.284000-284999.json\n",
            "Reading: mpd.slice.285000-285999.json\n",
            "Reading: mpd.slice.286000-286999.json\n",
            "Reading: mpd.slice.287000-287999.json\n",
            "Reading: mpd.slice.288000-288999.json\n",
            "Reading: mpd.slice.289000-289999.json\n",
            "Reading: mpd.slice.290000-290999.json\n",
            "Reading: mpd.slice.291000-291999.json\n",
            "Reading: mpd.slice.292000-292999.json\n",
            "Reading: mpd.slice.293000-293999.json\n",
            "Reading: mpd.slice.294000-294999.json\n",
            "Reading: mpd.slice.295000-295999.json\n",
            "Reading: mpd.slice.296000-296999.json\n",
            "Reading: mpd.slice.297000-297999.json\n",
            "Reading: mpd.slice.298000-298999.json\n",
            "Reading: mpd.slice.299000-299999.json\n",
            "Reading: mpd.slice.300000-300999.json\n",
            "Reading: mpd.slice.301000-301999.json\n",
            "Reading: mpd.slice.302000-302999.json\n",
            "Reading: mpd.slice.303000-303999.json\n",
            "Reading: mpd.slice.304000-304999.json\n",
            "Reading: mpd.slice.305000-305999.json\n",
            "Reading: mpd.slice.306000-306999.json\n",
            "Reading: mpd.slice.307000-307999.json\n",
            "Reading: mpd.slice.308000-308999.json\n",
            "Reading: mpd.slice.309000-309999.json\n",
            "Reading: mpd.slice.310000-310999.json\n",
            "Reading: mpd.slice.311000-311999.json\n",
            "Reading: mpd.slice.312000-312999.json\n",
            "Reading: mpd.slice.313000-313999.json\n",
            "Reading: mpd.slice.314000-314999.json\n",
            "Reading: mpd.slice.315000-315999.json\n",
            "Reading: mpd.slice.316000-316999.json\n",
            "Reading: mpd.slice.317000-317999.json\n",
            "Reading: mpd.slice.318000-318999.json\n",
            "Reading: mpd.slice.319000-319999.json\n",
            "Reading: mpd.slice.320000-320999.json\n",
            "Reading: mpd.slice.321000-321999.json\n",
            "Reading: mpd.slice.322000-322999.json\n",
            "Reading: mpd.slice.323000-323999.json\n",
            "Reading: mpd.slice.324000-324999.json\n",
            "Reading: mpd.slice.325000-325999.json\n",
            "Reading: mpd.slice.326000-326999.json\n",
            "Reading: mpd.slice.327000-327999.json\n",
            "Reading: mpd.slice.328000-328999.json\n",
            "Reading: mpd.slice.329000-329999.json\n",
            "Reading: mpd.slice.330000-330999.json\n",
            "Reading: mpd.slice.331000-331999.json\n",
            "Reading: mpd.slice.332000-332999.json\n",
            "Reading: mpd.slice.333000-333999.json\n",
            "Reading: mpd.slice.334000-334999.json\n",
            "Reading: mpd.slice.335000-335999.json\n",
            "Reading: mpd.slice.336000-336999.json\n",
            "Reading: mpd.slice.337000-337999.json\n",
            "Reading: mpd.slice.338000-338999.json\n",
            "Reading: mpd.slice.339000-339999.json\n",
            "Reading: mpd.slice.340000-340999.json\n",
            "Reading: mpd.slice.341000-341999.json\n",
            "Reading: mpd.slice.342000-342999.json\n",
            "Reading: mpd.slice.343000-343999.json\n",
            "Reading: mpd.slice.344000-344999.json\n",
            "Reading: mpd.slice.345000-345999.json\n",
            "Reading: mpd.slice.346000-346999.json\n",
            "Reading: mpd.slice.347000-347999.json\n",
            "Reading: mpd.slice.348000-348999.json\n",
            "Reading: mpd.slice.349000-349999.json\n",
            "Reading: mpd.slice.350000-350999.json\n",
            "Reading: mpd.slice.351000-351999.json\n",
            "Reading: mpd.slice.352000-352999.json\n",
            "Reading: mpd.slice.353000-353999.json\n",
            "Reading: mpd.slice.354000-354999.json\n",
            "Reading: mpd.slice.355000-355999.json\n",
            "Reading: mpd.slice.356000-356999.json\n",
            "Reading: mpd.slice.357000-357999.json\n",
            "Reading: mpd.slice.358000-358999.json\n",
            "Reading: mpd.slice.359000-359999.json\n",
            "Reading: mpd.slice.360000-360999.json\n",
            "Reading: mpd.slice.361000-361999.json\n",
            "Reading: mpd.slice.362000-362999.json\n",
            "Reading: mpd.slice.363000-363999.json\n",
            "Reading: mpd.slice.364000-364999.json\n",
            "Reading: mpd.slice.365000-365999.json\n",
            "Reading: mpd.slice.366000-366999.json\n",
            "Reading: mpd.slice.367000-367999.json\n",
            "Reading: mpd.slice.368000-368999.json\n",
            "Reading: mpd.slice.369000-369999.json\n",
            "Reading: mpd.slice.370000-370999.json\n",
            "Reading: mpd.slice.371000-371999.json\n",
            "Reading: mpd.slice.372000-372999.json\n",
            "Reading: mpd.slice.373000-373999.json\n",
            "Reading: mpd.slice.374000-374999.json\n",
            "Reading: mpd.slice.375000-375999.json\n",
            "Reading: mpd.slice.376000-376999.json\n",
            "Reading: mpd.slice.377000-377999.json\n",
            "Reading: mpd.slice.378000-378999.json\n",
            "Reading: mpd.slice.379000-379999.json\n",
            "Reading: mpd.slice.380000-380999.json\n",
            "Reading: mpd.slice.381000-381999.json\n",
            "Reading: mpd.slice.382000-382999.json\n",
            "Reading: mpd.slice.383000-383999.json\n",
            "Reading: mpd.slice.384000-384999.json\n",
            "Reading: mpd.slice.385000-385999.json\n",
            "Reading: mpd.slice.386000-386999.json\n",
            "Reading: mpd.slice.387000-387999.json\n",
            "Reading: mpd.slice.388000-388999.json\n",
            "Reading: mpd.slice.389000-389999.json\n",
            "Reading: mpd.slice.390000-390999.json\n",
            "Reading: mpd.slice.391000-391999.json\n",
            "Reading: mpd.slice.392000-392999.json\n",
            "Reading: mpd.slice.393000-393999.json\n",
            "Reading: mpd.slice.394000-394999.json\n",
            "Reading: mpd.slice.395000-395999.json\n",
            "Reading: mpd.slice.396000-396999.json\n",
            "Reading: mpd.slice.397000-397999.json\n",
            "Reading: mpd.slice.398000-398999.json\n",
            "Reading: mpd.slice.399000-399999.json\n",
            "Processing file: mpd.slice.0-999.json\n",
            "Processing file: mpd.slice.1000-1999.json\n",
            "Processing file: mpd.slice.2000-2999.json\n",
            "Processing file: mpd.slice.3000-3999.json\n",
            "Processing file: mpd.slice.4000-4999.json\n",
            "Processing file: mpd.slice.5000-5999.json\n",
            "Processing file: mpd.slice.6000-6999.json\n",
            "Processing file: mpd.slice.7000-7999.json\n",
            "Processing file: mpd.slice.8000-8999.json\n",
            "Processing file: mpd.slice.9000-9999.json\n",
            "Processing file: mpd.slice.10000-10999.json\n",
            "Processing file: mpd.slice.11000-11999.json\n",
            "Processing file: mpd.slice.12000-12999.json\n",
            "Processing file: mpd.slice.13000-13999.json\n",
            "Processing file: mpd.slice.14000-14999.json\n",
            "Processing file: mpd.slice.15000-15999.json\n",
            "Processing file: mpd.slice.16000-16999.json\n",
            "Processing file: mpd.slice.17000-17999.json\n",
            "Processing file: mpd.slice.18000-18999.json\n",
            "Processing file: mpd.slice.19000-19999.json\n",
            "Processing file: mpd.slice.20000-20999.json\n",
            "Processing file: mpd.slice.21000-21999.json\n",
            "Processing file: mpd.slice.22000-22999.json\n",
            "Processing file: mpd.slice.23000-23999.json\n",
            "Processing file: mpd.slice.24000-24999.json\n",
            "Processing file: mpd.slice.25000-25999.json\n",
            "Processing file: mpd.slice.26000-26999.json\n",
            "Processing file: mpd.slice.27000-27999.json\n",
            "Processing file: mpd.slice.28000-28999.json\n",
            "Processing file: mpd.slice.29000-29999.json\n",
            "Processing file: mpd.slice.30000-30999.json\n",
            "Processing file: mpd.slice.31000-31999.json\n",
            "Processing file: mpd.slice.32000-32999.json\n",
            "Processing file: mpd.slice.33000-33999.json\n",
            "Processing file: mpd.slice.34000-34999.json\n",
            "Processing file: mpd.slice.35000-35999.json\n",
            "Processing file: mpd.slice.36000-36999.json\n",
            "Processing file: mpd.slice.37000-37999.json\n",
            "Processing file: mpd.slice.38000-38999.json\n",
            "Processing file: mpd.slice.39000-39999.json\n",
            "Processing file: mpd.slice.40000-40999.json\n",
            "Processing file: mpd.slice.41000-41999.json\n",
            "Processing file: mpd.slice.42000-42999.json\n",
            "Processing file: mpd.slice.43000-43999.json\n",
            "Processing file: mpd.slice.44000-44999.json\n",
            "Processing file: mpd.slice.45000-45999.json\n",
            "Processing file: mpd.slice.46000-46999.json\n",
            "Processing file: mpd.slice.47000-47999.json\n",
            "Processing file: mpd.slice.48000-48999.json\n",
            "Processing file: mpd.slice.49000-49999.json\n",
            "Processing file: mpd.slice.50000-50999.json\n",
            "Processing file: mpd.slice.51000-51999.json\n",
            "Processing file: mpd.slice.52000-52999.json\n",
            "Processing file: mpd.slice.53000-53999.json\n",
            "Processing file: mpd.slice.54000-54999.json\n",
            "Processing file: mpd.slice.55000-55999.json\n",
            "Processing file: mpd.slice.56000-56999.json\n",
            "Processing file: mpd.slice.57000-57999.json\n",
            "Processing file: mpd.slice.58000-58999.json\n",
            "Processing file: mpd.slice.59000-59999.json\n",
            "Processing file: mpd.slice.60000-60999.json\n",
            "Processing file: mpd.slice.61000-61999.json\n",
            "Processing file: mpd.slice.62000-62999.json\n",
            "Processing file: mpd.slice.63000-63999.json\n",
            "Processing file: mpd.slice.64000-64999.json\n",
            "Processing file: mpd.slice.65000-65999.json\n",
            "Processing file: mpd.slice.66000-66999.json\n",
            "Processing file: mpd.slice.67000-67999.json\n",
            "Processing file: mpd.slice.68000-68999.json\n",
            "Processing file: mpd.slice.69000-69999.json\n",
            "Processing file: mpd.slice.70000-70999.json\n",
            "Processing file: mpd.slice.71000-71999.json\n",
            "Processing file: mpd.slice.72000-72999.json\n",
            "Processing file: mpd.slice.73000-73999.json\n",
            "Processing file: mpd.slice.74000-74999.json\n",
            "Processing file: mpd.slice.75000-75999.json\n",
            "Processing file: mpd.slice.76000-76999.json\n",
            "Processing file: mpd.slice.77000-77999.json\n",
            "Processing file: mpd.slice.78000-78999.json\n",
            "Processing file: mpd.slice.79000-79999.json\n",
            "Processing file: mpd.slice.80000-80999.json\n",
            "Processing file: mpd.slice.81000-81999.json\n",
            "Processing file: mpd.slice.82000-82999.json\n",
            "Processing file: mpd.slice.83000-83999.json\n",
            "Processing file: mpd.slice.84000-84999.json\n",
            "Processing file: mpd.slice.85000-85999.json\n",
            "Processing file: mpd.slice.86000-86999.json\n",
            "Processing file: mpd.slice.87000-87999.json\n",
            "Processing file: mpd.slice.88000-88999.json\n",
            "Processing file: mpd.slice.89000-89999.json\n",
            "Processing file: mpd.slice.90000-90999.json\n",
            "Processing file: mpd.slice.91000-91999.json\n",
            "Processing file: mpd.slice.92000-92999.json\n",
            "Processing file: mpd.slice.93000-93999.json\n",
            "Processing file: mpd.slice.94000-94999.json\n",
            "Processing file: mpd.slice.95000-95999.json\n",
            "Processing file: mpd.slice.96000-96999.json\n",
            "Processing file: mpd.slice.97000-97999.json\n",
            "Processing file: mpd.slice.98000-98999.json\n",
            "Processing file: mpd.slice.99000-99999.json\n",
            "Processing file: mpd.slice.100000-100999.json\n",
            "Processing file: mpd.slice.101000-101999.json\n",
            "Processing file: mpd.slice.102000-102999.json\n",
            "Processing file: mpd.slice.103000-103999.json\n",
            "Processing file: mpd.slice.104000-104999.json\n",
            "Processing file: mpd.slice.105000-105999.json\n",
            "Processing file: mpd.slice.106000-106999.json\n",
            "Processing file: mpd.slice.107000-107999.json\n",
            "Processing file: mpd.slice.108000-108999.json\n",
            "Processing file: mpd.slice.109000-109999.json\n",
            "Processing file: mpd.slice.110000-110999.json\n",
            "Processing file: mpd.slice.111000-111999.json\n",
            "Processing file: mpd.slice.112000-112999.json\n",
            "Processing file: mpd.slice.113000-113999.json\n",
            "Processing file: mpd.slice.114000-114999.json\n",
            "Processing file: mpd.slice.115000-115999.json\n",
            "Processing file: mpd.slice.116000-116999.json\n",
            "Processing file: mpd.slice.117000-117999.json\n",
            "Processing file: mpd.slice.118000-118999.json\n",
            "Processing file: mpd.slice.119000-119999.json\n",
            "Processing file: mpd.slice.120000-120999.json\n",
            "Processing file: mpd.slice.121000-121999.json\n",
            "Processing file: mpd.slice.122000-122999.json\n",
            "Processing file: mpd.slice.123000-123999.json\n",
            "Processing file: mpd.slice.124000-124999.json\n",
            "Processing file: mpd.slice.125000-125999.json\n",
            "Processing file: mpd.slice.126000-126999.json\n",
            "Processing file: mpd.slice.127000-127999.json\n",
            "Processing file: mpd.slice.128000-128999.json\n",
            "Processing file: mpd.slice.129000-129999.json\n",
            "Processing file: mpd.slice.130000-130999.json\n",
            "Processing file: mpd.slice.131000-131999.json\n",
            "Processing file: mpd.slice.132000-132999.json\n",
            "Processing file: mpd.slice.133000-133999.json\n",
            "Processing file: mpd.slice.134000-134999.json\n",
            "Processing file: mpd.slice.135000-135999.json\n",
            "Processing file: mpd.slice.136000-136999.json\n",
            "Processing file: mpd.slice.137000-137999.json\n",
            "Processing file: mpd.slice.138000-138999.json\n",
            "Processing file: mpd.slice.139000-139999.json\n",
            "Processing file: mpd.slice.140000-140999.json\n",
            "Processing file: mpd.slice.141000-141999.json\n",
            "Processing file: mpd.slice.142000-142999.json\n",
            "Processing file: mpd.slice.143000-143999.json\n",
            "Processing file: mpd.slice.144000-144999.json\n",
            "Processing file: mpd.slice.145000-145999.json\n",
            "Processing file: mpd.slice.146000-146999.json\n",
            "Processing file: mpd.slice.147000-147999.json\n",
            "Processing file: mpd.slice.148000-148999.json\n",
            "Processing file: mpd.slice.149000-149999.json\n",
            "Processing file: mpd.slice.150000-150999.json\n",
            "Processing file: mpd.slice.151000-151999.json\n",
            "Processing file: mpd.slice.152000-152999.json\n",
            "Processing file: mpd.slice.153000-153999.json\n",
            "Processing file: mpd.slice.154000-154999.json\n",
            "Processing file: mpd.slice.155000-155999.json\n",
            "Processing file: mpd.slice.156000-156999.json\n",
            "Processing file: mpd.slice.157000-157999.json\n",
            "Processing file: mpd.slice.158000-158999.json\n",
            "Processing file: mpd.slice.159000-159999.json\n",
            "Processing file: mpd.slice.160000-160999.json\n",
            "Processing file: mpd.slice.161000-161999.json\n",
            "Processing file: mpd.slice.162000-162999.json\n",
            "Processing file: mpd.slice.163000-163999.json\n",
            "Processing file: mpd.slice.164000-164999.json\n",
            "Processing file: mpd.slice.165000-165999.json\n",
            "Processing file: mpd.slice.166000-166999.json\n",
            "Processing file: mpd.slice.167000-167999.json\n",
            "Processing file: mpd.slice.168000-168999.json\n",
            "Processing file: mpd.slice.169000-169999.json\n",
            "Processing file: mpd.slice.170000-170999.json\n",
            "Processing file: mpd.slice.171000-171999.json\n",
            "Processing file: mpd.slice.172000-172999.json\n",
            "Processing file: mpd.slice.173000-173999.json\n",
            "Processing file: mpd.slice.174000-174999.json\n",
            "Processing file: mpd.slice.175000-175999.json\n",
            "Processing file: mpd.slice.176000-176999.json\n",
            "Processing file: mpd.slice.177000-177999.json\n",
            "Processing file: mpd.slice.178000-178999.json\n",
            "Processing file: mpd.slice.179000-179999.json\n",
            "Processing file: mpd.slice.180000-180999.json\n",
            "Processing file: mpd.slice.181000-181999.json\n",
            "Processing file: mpd.slice.182000-182999.json\n",
            "Processing file: mpd.slice.183000-183999.json\n",
            "Processing file: mpd.slice.184000-184999.json\n",
            "Processing file: mpd.slice.185000-185999.json\n",
            "Processing file: mpd.slice.186000-186999.json\n",
            "Processing file: mpd.slice.187000-187999.json\n",
            "Processing file: mpd.slice.188000-188999.json\n",
            "Processing file: mpd.slice.189000-189999.json\n",
            "Processing file: mpd.slice.190000-190999.json\n",
            "Processing file: mpd.slice.191000-191999.json\n",
            "Processing file: mpd.slice.192000-192999.json\n",
            "Processing file: mpd.slice.193000-193999.json\n",
            "Processing file: mpd.slice.194000-194999.json\n",
            "Processing file: mpd.slice.195000-195999.json\n",
            "Processing file: mpd.slice.196000-196999.json\n",
            "Processing file: mpd.slice.197000-197999.json\n",
            "Processing file: mpd.slice.198000-198999.json\n",
            "Processing file: mpd.slice.199000-199999.json\n",
            "Processing file: mpd.slice.200000-200999.json\n",
            "Processing file: mpd.slice.201000-201999.json\n",
            "Processing file: mpd.slice.202000-202999.json\n",
            "Processing file: mpd.slice.203000-203999.json\n",
            "Processing file: mpd.slice.204000-204999.json\n",
            "Processing file: mpd.slice.205000-205999.json\n",
            "Processing file: mpd.slice.206000-206999.json\n",
            "Processing file: mpd.slice.207000-207999.json\n",
            "Processing file: mpd.slice.208000-208999.json\n",
            "Processing file: mpd.slice.209000-209999.json\n",
            "Processing file: mpd.slice.210000-210999.json\n",
            "Processing file: mpd.slice.211000-211999.json\n",
            "Processing file: mpd.slice.212000-212999.json\n",
            "Processing file: mpd.slice.213000-213999.json\n",
            "Processing file: mpd.slice.214000-214999.json\n",
            "Processing file: mpd.slice.215000-215999.json\n",
            "Processing file: mpd.slice.216000-216999.json\n",
            "Processing file: mpd.slice.217000-217999.json\n",
            "Processing file: mpd.slice.218000-218999.json\n",
            "Processing file: mpd.slice.219000-219999.json\n",
            "Processing file: mpd.slice.220000-220999.json\n",
            "Processing file: mpd.slice.221000-221999.json\n",
            "Processing file: mpd.slice.222000-222999.json\n",
            "Processing file: mpd.slice.223000-223999.json\n",
            "Processing file: mpd.slice.224000-224999.json\n",
            "Processing file: mpd.slice.225000-225999.json\n",
            "Processing file: mpd.slice.226000-226999.json\n",
            "Processing file: mpd.slice.227000-227999.json\n",
            "Processing file: mpd.slice.228000-228999.json\n",
            "Processing file: mpd.slice.229000-229999.json\n",
            "Processing file: mpd.slice.230000-230999.json\n",
            "Processing file: mpd.slice.231000-231999.json\n",
            "Processing file: mpd.slice.232000-232999.json\n",
            "Processing file: mpd.slice.233000-233999.json\n",
            "Processing file: mpd.slice.234000-234999.json\n",
            "Processing file: mpd.slice.235000-235999.json\n",
            "Processing file: mpd.slice.236000-236999.json\n",
            "Processing file: mpd.slice.237000-237999.json\n",
            "Processing file: mpd.slice.238000-238999.json\n",
            "Processing file: mpd.slice.239000-239999.json\n",
            "Processing file: mpd.slice.240000-240999.json\n",
            "Processing file: mpd.slice.241000-241999.json\n",
            "Processing file: mpd.slice.242000-242999.json\n",
            "Processing file: mpd.slice.243000-243999.json\n",
            "Processing file: mpd.slice.244000-244999.json\n",
            "Processing file: mpd.slice.245000-245999.json\n",
            "Processing file: mpd.slice.246000-246999.json\n",
            "Processing file: mpd.slice.247000-247999.json\n",
            "Processing file: mpd.slice.248000-248999.json\n",
            "Processing file: mpd.slice.249000-249999.json\n",
            "Processing file: mpd.slice.250000-250999.json\n",
            "Processing file: mpd.slice.251000-251999.json\n",
            "Processing file: mpd.slice.252000-252999.json\n",
            "Processing file: mpd.slice.253000-253999.json\n",
            "Processing file: mpd.slice.254000-254999.json\n",
            "Processing file: mpd.slice.255000-255999.json\n",
            "Processing file: mpd.slice.256000-256999.json\n",
            "Processing file: mpd.slice.257000-257999.json\n",
            "Processing file: mpd.slice.258000-258999.json\n",
            "Processing file: mpd.slice.259000-259999.json\n",
            "Processing file: mpd.slice.260000-260999.json\n",
            "Processing file: mpd.slice.261000-261999.json\n",
            "Processing file: mpd.slice.262000-262999.json\n",
            "Processing file: mpd.slice.263000-263999.json\n",
            "Processing file: mpd.slice.264000-264999.json\n",
            "Processing file: mpd.slice.265000-265999.json\n",
            "Processing file: mpd.slice.266000-266999.json\n",
            "Processing file: mpd.slice.267000-267999.json\n",
            "Processing file: mpd.slice.268000-268999.json\n",
            "Processing file: mpd.slice.269000-269999.json\n",
            "Processing file: mpd.slice.270000-270999.json\n",
            "Processing file: mpd.slice.271000-271999.json\n",
            "Processing file: mpd.slice.272000-272999.json\n",
            "Processing file: mpd.slice.273000-273999.json\n",
            "Processing file: mpd.slice.274000-274999.json\n",
            "Processing file: mpd.slice.275000-275999.json\n",
            "Processing file: mpd.slice.276000-276999.json\n",
            "Processing file: mpd.slice.277000-277999.json\n",
            "Processing file: mpd.slice.278000-278999.json\n",
            "Processing file: mpd.slice.279000-279999.json\n",
            "Processing file: mpd.slice.280000-280999.json\n",
            "Processing file: mpd.slice.281000-281999.json\n",
            "Processing file: mpd.slice.282000-282999.json\n",
            "Processing file: mpd.slice.283000-283999.json\n",
            "Processing file: mpd.slice.284000-284999.json\n",
            "Processing file: mpd.slice.285000-285999.json\n",
            "Processing file: mpd.slice.286000-286999.json\n",
            "Processing file: mpd.slice.287000-287999.json\n",
            "Processing file: mpd.slice.288000-288999.json\n",
            "Processing file: mpd.slice.289000-289999.json\n",
            "Processing file: mpd.slice.290000-290999.json\n",
            "Processing file: mpd.slice.291000-291999.json\n",
            "Processing file: mpd.slice.292000-292999.json\n",
            "Processing file: mpd.slice.293000-293999.json\n",
            "Processing file: mpd.slice.294000-294999.json\n",
            "Processing file: mpd.slice.295000-295999.json\n",
            "Processing file: mpd.slice.296000-296999.json\n",
            "Processing file: mpd.slice.297000-297999.json\n",
            "Processing file: mpd.slice.298000-298999.json\n",
            "Processing file: mpd.slice.299000-299999.json\n",
            "Processing file: mpd.slice.300000-300999.json\n",
            "Processing file: mpd.slice.301000-301999.json\n",
            "Processing file: mpd.slice.302000-302999.json\n",
            "Processing file: mpd.slice.303000-303999.json\n",
            "Processing file: mpd.slice.304000-304999.json\n",
            "Processing file: mpd.slice.305000-305999.json\n",
            "Processing file: mpd.slice.306000-306999.json\n",
            "Processing file: mpd.slice.307000-307999.json\n",
            "Processing file: mpd.slice.308000-308999.json\n",
            "Processing file: mpd.slice.309000-309999.json\n",
            "Processing file: mpd.slice.310000-310999.json\n",
            "Processing file: mpd.slice.311000-311999.json\n",
            "Processing file: mpd.slice.312000-312999.json\n",
            "Processing file: mpd.slice.313000-313999.json\n",
            "Processing file: mpd.slice.314000-314999.json\n",
            "Processing file: mpd.slice.315000-315999.json\n",
            "Processing file: mpd.slice.316000-316999.json\n",
            "Processing file: mpd.slice.317000-317999.json\n",
            "Processing file: mpd.slice.318000-318999.json\n",
            "Processing file: mpd.slice.319000-319999.json\n",
            "Processing file: mpd.slice.320000-320999.json\n",
            "Processing file: mpd.slice.321000-321999.json\n",
            "Processing file: mpd.slice.322000-322999.json\n",
            "Processing file: mpd.slice.323000-323999.json\n",
            "Processing file: mpd.slice.324000-324999.json\n",
            "Processing file: mpd.slice.325000-325999.json\n",
            "Processing file: mpd.slice.326000-326999.json\n",
            "Processing file: mpd.slice.327000-327999.json\n",
            "Processing file: mpd.slice.328000-328999.json\n",
            "Processing file: mpd.slice.329000-329999.json\n",
            "Processing file: mpd.slice.330000-330999.json\n",
            "Processing file: mpd.slice.331000-331999.json\n",
            "Processing file: mpd.slice.332000-332999.json\n",
            "Processing file: mpd.slice.333000-333999.json\n",
            "Processing file: mpd.slice.334000-334999.json\n",
            "Processing file: mpd.slice.335000-335999.json\n",
            "Processing file: mpd.slice.336000-336999.json\n",
            "Processing file: mpd.slice.337000-337999.json\n",
            "Processing file: mpd.slice.338000-338999.json\n",
            "Processing file: mpd.slice.339000-339999.json\n",
            "Processing file: mpd.slice.340000-340999.json\n",
            "Processing file: mpd.slice.341000-341999.json\n",
            "Processing file: mpd.slice.342000-342999.json\n",
            "Processing file: mpd.slice.343000-343999.json\n",
            "Processing file: mpd.slice.344000-344999.json\n",
            "Processing file: mpd.slice.345000-345999.json\n",
            "Processing file: mpd.slice.346000-346999.json\n",
            "Processing file: mpd.slice.347000-347999.json\n",
            "Processing file: mpd.slice.348000-348999.json\n",
            "Processing file: mpd.slice.349000-349999.json\n",
            "Processing file: mpd.slice.350000-350999.json\n",
            "Processing file: mpd.slice.351000-351999.json\n",
            "Processing file: mpd.slice.352000-352999.json\n",
            "Processing file: mpd.slice.353000-353999.json\n",
            "Processing file: mpd.slice.354000-354999.json\n",
            "Processing file: mpd.slice.355000-355999.json\n",
            "Processing file: mpd.slice.356000-356999.json\n",
            "Processing file: mpd.slice.357000-357999.json\n",
            "Processing file: mpd.slice.358000-358999.json\n",
            "Processing file: mpd.slice.359000-359999.json\n",
            "Processing file: mpd.slice.360000-360999.json\n",
            "Processing file: mpd.slice.361000-361999.json\n",
            "Processing file: mpd.slice.362000-362999.json\n",
            "Processing file: mpd.slice.363000-363999.json\n",
            "Processing file: mpd.slice.364000-364999.json\n",
            "Processing file: mpd.slice.365000-365999.json\n",
            "Processing file: mpd.slice.366000-366999.json\n",
            "Processing file: mpd.slice.367000-367999.json\n",
            "Processing file: mpd.slice.368000-368999.json\n",
            "Processing file: mpd.slice.369000-369999.json\n",
            "Processing file: mpd.slice.370000-370999.json\n",
            "Processing file: mpd.slice.371000-371999.json\n",
            "Processing file: mpd.slice.372000-372999.json\n",
            "Processing file: mpd.slice.373000-373999.json\n",
            "Processing file: mpd.slice.374000-374999.json\n",
            "Processing file: mpd.slice.375000-375999.json\n",
            "Processing file: mpd.slice.376000-376999.json\n",
            "Processing file: mpd.slice.377000-377999.json\n",
            "Processing file: mpd.slice.378000-378999.json\n",
            "Processing file: mpd.slice.379000-379999.json\n",
            "Processing file: mpd.slice.380000-380999.json\n",
            "Processing file: mpd.slice.381000-381999.json\n",
            "Processing file: mpd.slice.382000-382999.json\n",
            "Processing file: mpd.slice.383000-383999.json\n",
            "Processing file: mpd.slice.384000-384999.json\n",
            "Processing file: mpd.slice.385000-385999.json\n",
            "Processing file: mpd.slice.386000-386999.json\n",
            "Processing file: mpd.slice.387000-387999.json\n",
            "Processing file: mpd.slice.388000-388999.json\n",
            "Processing file: mpd.slice.389000-389999.json\n",
            "Processing file: mpd.slice.390000-390999.json\n",
            "Processing file: mpd.slice.391000-391999.json\n",
            "Processing file: mpd.slice.392000-392999.json\n",
            "Processing file: mpd.slice.393000-393999.json\n",
            "Processing file: mpd.slice.394000-394999.json\n",
            "Processing file: mpd.slice.395000-395999.json\n",
            "Processing file: mpd.slice.396000-396999.json\n",
            "Processing file: mpd.slice.397000-397999.json\n",
            "Processing file: mpd.slice.398000-398999.json\n",
            "Processing file: mpd.slice.399000-399999.json\n",
            "Graph with 1836460 nodes and 26207600 edges\n",
            "Graph with 570402 nodes and 23427997 edges\n"
          ]
        }
      ],
      "source": [
        "# in thousands remember? files as in actual files\n",
        "NUM_FILES_TO_KEEP = 400\n",
        "KCORE = 10\n",
        "playlist_G_NX = NXPlaylistGraph(DATA_DIR, NUM_FILES_TO_KEEP, KCORE)\n",
        "playlist_G_NX.generate_graph()\n",
        "# playlist_G_NX.encode_graph()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e31ee5fd-55fa-4e33-bf95-2378680a2937",
      "metadata": {
        "id": "e31ee5fd-55fa-4e33-bf95-2378680a2937",
        "outputId": "0c2e6685-832b-46da-c57f-d5cb54b605dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num nodes: 570402 . Num edges: 23427997\n"
          ]
        }
      ],
      "source": [
        "G_NX = playlist_G_NX.get_graph()\n",
        "print('Num nodes:', G_NX.number_of_nodes(), '. Num edges:', G_NX.number_of_edges())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42728df6-11d7-4c28-a6d8-d34314222aeb",
      "metadata": {
        "id": "42728df6-11d7-4c28-a6d8-d34314222aeb"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Since we have different node types with mix namings (letters + numbers)\n",
        "# Help sorting the nodes with priority + numerical order\n",
        "def custom_node_sort(node):\n",
        "  \"\"\"\n",
        "  Custom sorting function to handle 4 node types and numerical sorting:\n",
        "  1. playlist\n",
        "  2. album\n",
        "  3. artist\n",
        "  4. track\n",
        "  \"\"\"\n",
        "  node_type = G_NX.nodes[node]['type']\n",
        "  type_priority = {\n",
        "      'playlist': 1,\n",
        "      'track': 2\n",
        "  }\n",
        "\n",
        "  # Extract numerical part using regular expression\n",
        "  numerical_part = int(re.findall(r'\\d+', node)[0]) if re.findall(r'\\d+', node) else 0\n",
        "\n",
        "  return (type_priority[node_type], numerical_part, node)  # Sort by type, numerical part, then node name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2556c62-fa61-4f5f-a8ae-f79d3c4e5617",
      "metadata": {
        "id": "f2556c62-fa61-4f5f-a8ae-f79d3c4e5617",
        "outputId": "6571431c-cf19-48f5-8a67-de9db198d3a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "570402\n",
            "23427997\n"
          ]
        }
      ],
      "source": [
        "n_nodes, n_edges = G_NX.number_of_nodes(), G_NX.number_of_edges()\n",
        "print(n_nodes)\n",
        "print(n_edges)\n",
        "# Sort nodes inside the initial graph, currently in order of playlist, album, artist, track, and with numerical portion ordered in numerical order\n",
        "sorted_nodes = sorted(list(G_NX.nodes()), key=custom_node_sort)\n",
        "\n",
        "# create dictionaries to index to 0 to n_nodes, will be necessary for when we are using tensors\n",
        "# node2id maps/labels each node with indices from 0 to # of nodes - 1\n",
        "# id2node maps/labels back each node index to correlated node types\n",
        "node2id = dict(zip(sorted_nodes, np.arange(n_nodes)))\n",
        "id2node = dict(zip(np.arange(n_nodes), sorted_nodes))\n",
        "\n",
        "# a relabeled graph with each node type replaced with assigned indices\n",
        "G_NX_sorted = nx.relabel_nodes(G_NX, node2id)\n",
        "\n",
        "# sort indices by the origianl node type\n",
        "playlists_idx = [i for i, v in enumerate(node2id.keys()) if \"playlist\" in v]\n",
        "tracks_idx = [i for i, v in enumerate(node2id.keys()) if \"track\" in v]\n",
        "# artists_idx = [i for i, v in enumerate(node2id.keys()) if \"artist\" in v]\n",
        "# albums_idx = [i for i, v in enumerate(node2id.keys()) if \"album\" in v]\n",
        "\n",
        "n_playlists = len(playlists_idx)\n",
        "n_tracks = len(tracks_idx)\n",
        "# n_artists = len(artists_idx)\n",
        "# n_albums = len(albums_idx)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3ddcf76-3e93-48fe-aa37-685c14dc94d6",
      "metadata": {
        "id": "d3ddcf76-3e93-48fe-aa37-685c14dc94d6",
        "outputId": "28b5c4d2-2717-452e-e4df-3e38e94c2d0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Edge index shape: torch.Size([2, 23427997])\n",
            "tensor([423918., 462226., 395689.,  ..., 248481., 459215., 531213.])\n"
          ]
        }
      ],
      "source": [
        "# Now prepare message passsing edges + supervision edges, and\n",
        "# Map the networkx graph to pytorch geometric Data object\n",
        "num_nodes = n_playlists + n_tracks\n",
        "edge_idx = torch.Tensor(np.array(G_NX_sorted.edges()).T)\n",
        "graph_data = Data(edge_index = edge_idx, num_nodes = num_nodes)\n",
        "print(\"Edge index shape:\", edge_idx.shape)\n",
        "print(edge_idx[1])\n",
        "\n",
        "# Dictionary to access different edges by types\n",
        "# Note that edges are stored as indicies in the format of (source,destination)\n",
        "# Use id2node to map it back to exact nodes\n",
        "edge_list = {\n",
        "  'artist_to_album': [],\n",
        "  'album_to_track': [],\n",
        "  'track_to_playlist': [],\n",
        "  'artist_to_track': [],\n",
        "  'album_to_playlist': [],\n",
        "  'playlist_to_artist': [],\n",
        "}\n",
        "\n",
        "for source, dest, data in G_NX_sorted.edges(data=True):\n",
        "    edge_list[data['edge_type']].append((source, dest))\n",
        "\n",
        "# Use RandomLinkSplit to randomly remove the x% of edges from graph\n",
        "# Ensures no duplicate edges between datasets\n",
        "transform_split = RandomLinkSplit(\n",
        "    is_undirected=True,\n",
        "    add_negative_train_samples=False,\n",
        "    neg_sampling_ratio=0,\n",
        "    num_val=0.2, num_test=0.1\n",
        ")\n",
        "\n",
        "train_split, val_split, test_split = transform_split(graph_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c672b1b5-981a-40b7-8cd0-f67db382dfcc",
      "metadata": {
        "id": "c672b1b5-981a-40b7-8cd0-f67db382dfcc",
        "outputId": "6cafe3a2-07c5-426f-dc82-6ad6664ed1b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train set has 8178565 positive supervision edges\n",
            "Validation set has 2336732 positive supervision edges\n",
            "Test set has 1168366 positive supervision edges\n",
            "Train set has 16357130 message passing edges\n",
            "Validation set has 16357130 message passing edges\n",
            "Test set has 21030594 message passing edges\n"
          ]
        }
      ],
      "source": [
        "# note these are stored as float32, we need them to be int64 for future training\n",
        "\n",
        "# Edge index: message passing edges\n",
        "train_split.edge_index = train_split.edge_index.type(torch.int64) # Contains message passing edge indicies for training\n",
        "val_split.edge_index = val_split.edge_index.type(torch.int64) # Contains message passing edge indicies for validation\n",
        "test_split.edge_index = test_split.edge_index.type(torch.int64) # Contains message passing edge indicies for testing\n",
        "\n",
        "# Edge label index: supervision edges\n",
        "train_split.edge_label_index = train_split.edge_label_index.type(torch.int64) # Contains supervision edge indicies for training\n",
        "val_split.edge_label_index = val_split.edge_label_index.type(torch.int64) # Contains supervision edge indicies for validation\n",
        "test_split.edge_label_index = test_split.edge_label_index.type(torch.int64) # Contains supervision edge indicies for testing\n",
        "\n",
        "print(f\"Train set has {train_split.edge_label_index.shape[1]} positive supervision edges\")\n",
        "print(f\"Validation set has {val_split.edge_label_index.shape[1]} positive supervision edges\")\n",
        "print(f\"Test set has {test_split.edge_label_index.shape[1]} positive supervision edges\")\n",
        "\n",
        "print(f\"Train set has {train_split.edge_index.shape[1]} message passing edges\")\n",
        "print(f\"Validation set has {val_split.edge_index.shape[1]} message passing edges\")\n",
        "print(f\"Test set has {test_split.edge_index.shape[1]} message passing edges\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d542a96-8d22-4c1a-be97-7848173cb006",
      "metadata": {
        "id": "2d542a96-8d22-4c1a-be97-7848173cb006"
      },
      "outputs": [],
      "source": [
        "def recall_at_k(data, model, k = 300, batch_size = 64, device = None):\n",
        "    with torch.no_grad():\n",
        "        embeddings = model.get_embedding(data.edge_index)\n",
        "        playlists_embeddings = embeddings[:n_playlists]\n",
        "        tracks_embeddings = embeddings[n_playlists:]\n",
        "\n",
        "    hits_list = []\n",
        "    relevant_counts_list = []\n",
        "\n",
        "    for batch_start in range(0, n_playlists, batch_size):\n",
        "        batch_end = min(batch_start + batch_size, n_playlists)\n",
        "        batch_playlists_embeddings = playlists_embeddings[batch_start:batch_end]\n",
        "\n",
        "        # Calculate scores for all possible item pairs\n",
        "        scores = torch.matmul(batch_playlists_embeddings, tracks_embeddings.t())\n",
        "\n",
        "        # Set the scores of message passing edges to negative infinity\n",
        "        mp_indices = ((data.edge_index[0] >= batch_start) & (data.edge_index[0] < batch_end)).nonzero(as_tuple=True)[0]\n",
        "        scores[data.edge_index[0, mp_indices] - batch_start, data.edge_index[1, mp_indices] - n_playlists] = -float(\"inf\")\n",
        "\n",
        "        # Find the top k highest scoring items for each playlist in the batch\n",
        "        _, top_k_indices = torch.topk(scores, k, dim=1)\n",
        "\n",
        "        # Ground truth supervision edges\n",
        "        ground_truth_edges = data.edge_label_index\n",
        "\n",
        "        # Create a mask to indicate if the top k items are in the ground truth supervision edges\n",
        "        mask = torch.zeros(scores.shape, device=device, dtype=torch.bool)\n",
        "        gt_indices = ((ground_truth_edges[0] >= batch_start) & (ground_truth_edges[0] < batch_end)).nonzero(as_tuple=True)[0]\n",
        "        mask[ground_truth_edges[0, gt_indices] - batch_start, ground_truth_edges[1, gt_indices] - n_playlists] = True\n",
        "\n",
        "        # Check how many of the top k items are in the ground truth supervision edges\n",
        "        hits = mask.gather(1, top_k_indices).sum(dim=1)\n",
        "        hits_list.append(hits)\n",
        "\n",
        "        # Calculate the total number of relevant items for each playlist in the batch\n",
        "        relevant_counts = torch.bincount(ground_truth_edges[0, gt_indices] - batch_start, minlength=batch_end - batch_start)\n",
        "        relevant_counts_list.append(relevant_counts)\n",
        "\n",
        "    # Compute recall@k\n",
        "    hits_tensor = torch.cat(hits_list, dim=0)\n",
        "    relevant_counts_tensor = torch.cat(relevant_counts_list, dim=0)\n",
        "    # Handle division by zero case\n",
        "    recall_at_k = torch.where(\n",
        "        relevant_counts_tensor != 0,\n",
        "        hits_tensor.true_divide(relevant_counts_tensor),\n",
        "        torch.ones_like(hits_tensor)\n",
        "    )\n",
        "    # take average\n",
        "    recall_at_k = torch.mean(recall_at_k)\n",
        "\n",
        "    if recall_at_k.numel() == 1:\n",
        "        return recall_at_k.item()\n",
        "    else:\n",
        "        raise ValueError(\"recall_at_k contains more than one item.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b44c13da-80e0-46cd-8515-798e693f2024",
      "metadata": {
        "id": "b44c13da-80e0-46cd-8515-798e693f2024"
      },
      "outputs": [],
      "source": [
        "def precision_at_k(data, model, k=300, batch_size=64, device=None):\n",
        "    with torch.no_grad():\n",
        "        embeddings = model.get_embedding(data.edge_index)\n",
        "        playlists_embeddings = embeddings[:n_playlists]\n",
        "        tracks_embeddings = embeddings[n_playlists:]\n",
        "\n",
        "    hits_list = []\n",
        "    relevant_counts_list = []\n",
        "\n",
        "    for batch_start in range(0, n_playlists, batch_size):\n",
        "        batch_end = min(batch_start + batch_size, n_playlists)\n",
        "        batch_playlists_embeddings = playlists_embeddings[batch_start:batch_end]\n",
        "\n",
        "        # Calculate scores for all possible item pairs\n",
        "        scores = torch.matmul(batch_playlists_embeddings, tracks_embeddings.t())\n",
        "\n",
        "        # Set the scores of message passing edges to negative infinity\n",
        "        mp_indices = ((data.edge_index[0] >= batch_start) & (data.edge_index[0] < batch_end)).nonzero(as_tuple=True)[0]\n",
        "        scores[data.edge_index[0, mp_indices] - batch_start, data.edge_index[1, mp_indices] - n_playlists] = -float(\"inf\")\n",
        "\n",
        "        # Find the top k highest scoring items for each playlist in the batch\n",
        "        _, top_k_indices = torch.topk(scores, k, dim=1)\n",
        "\n",
        "        # Ground truth supervision edges\n",
        "        ground_truth_edges = data.edge_label_index\n",
        "\n",
        "        # Create a mask to indicate if the top k items are in the ground truth supervision edges\n",
        "        mask = torch.zeros(scores.shape, device=device, dtype=torch.bool)\n",
        "        gt_indices = ((ground_truth_edges[0] >= batch_start) & (ground_truth_edges[0] < batch_end)).nonzero(as_tuple=True)[0]\n",
        "        mask[ground_truth_edges[0, gt_indices] - batch_start, ground_truth_edges[1, gt_indices] - n_playlists] = True\n",
        "\n",
        "        # Check how many of the top k items are in the ground truth supervision edges\n",
        "        hits = mask.gather(1, top_k_indices).sum(dim=1)\n",
        "        hits_list.append(hits)\n",
        "\n",
        "        # Precision at k for the batch is number of relevant items in top k divided by k\n",
        "        precision = hits / k\n",
        "        relevant_counts_list.append(precision)\n",
        "\n",
        "    # Compute average precision@k\n",
        "    precision_at_k = torch.cat(relevant_counts_list, dim=0)\n",
        "    precision_at_k = torch.mean(precision_at_k)\n",
        "\n",
        "    if precision_at_k.numel() == 1:\n",
        "        return precision_at_k.item()\n",
        "    else:\n",
        "        raise ValueError(\"precision_at_k contains more than one item.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57a2c14e-005e-4c41-8c5c-9e745a37740a",
      "metadata": {
        "id": "57a2c14e-005e-4c41-8c5c-9e745a37740a"
      },
      "outputs": [],
      "source": [
        "\n",
        "class GCN(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, num_nodes: int, embedding_dim: int, num_layers: int, alpha: Optional[Union[float, Tensor]] = None,\n",
        "        conv_layer = \"LGC\", name = None, **kwargs,):\n",
        "        super().__init__()\n",
        "        self.sigmoid = torch.sigmoid\n",
        "        self.conv_layer = conv_layer\n",
        "        self.name = f\"{self.conv_layer}_RUN5\"\n",
        "        alpha_string = \"alpha\"\n",
        "        self.num_nodes = num_nodes\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.alpha_vals = torch.rand(num_layers+1)\n",
        "        alpha = nn.Parameter(self.alpha_vals/torch.sum(self.alpha_vals))\n",
        "\n",
        "\n",
        "        self.register_buffer('alpha', alpha)\n",
        "\n",
        "        self.embedding = Embedding(num_nodes, embedding_dim)\n",
        "\n",
        "        # initialize convolutional layers\n",
        "        if conv_layer == \"LGC\":\n",
        "          self.convs = ModuleList([LGConv(**kwargs) for _ in range(num_layers)])\n",
        "        elif conv_layer == \"GAT\":\n",
        "          # initialize Graph Attention layer with multiple heads\n",
        "          # initialize linear layers to aggregate heads\n",
        "          n_heads = 3\n",
        "          self.convs = ModuleList(\n",
        "              [GATConv(in_channels = embedding_dim, out_channels = embedding_dim, heads = n_heads, dropout = 0.5, **kwargs) for _ in range(num_layers)]\n",
        "          )\n",
        "          self.linears = ModuleList([Linear(n_heads * embedding_dim, embedding_dim) for _ in range(num_layers)])\n",
        "        elif conv_layer == \"SAGE\":\n",
        "          #  initialize GraphSAGE conv\n",
        "          self.convs = ModuleList(\n",
        "              [SAGEConv(in_channels = embedding_dim, out_channels = embedding_dim, **kwargs) for _ in range(num_layers)]\n",
        "          )\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        torch.nn.init.xavier_uniform_(self.embedding.weight)\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "\n",
        "    def forward(self, edge_index: Adj, edge_label_index: OptTensor = None) -> Tensor:\n",
        "        # Default edge_label_index to edge_index if not provided\n",
        "        edge_label_index = edge_label_index or (torch.stack(edge_index.coo()[:2], dim=0) if isinstance(edge_index, SparseTensor) else edge_index)\n",
        "        out = self.get_embedding(edge_index)\n",
        "        return self.predict_link(out, edge_label_index)\n",
        "\n",
        "    def get_embedding(self, edge_index: Adj) -> Tensor:\n",
        "        x = self.embedding.weight\n",
        "        weights = self.alpha.softmax(dim=-1)\n",
        "        out = x * weights[0]\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.convs[i](x, edge_index)\n",
        "            if self.conv_layer == \"GAT\":\n",
        "                out = out + self.linears[i](x) * weights[i + 1]\n",
        "        return out\n",
        "\n",
        "    def predict_link(self, embed: Adj, edge_label_index: Adj) -> Tensor:\n",
        "        return (embed[edge_label_index[0]] * embed[edge_label_index[1]]).sum(dim=-1)\n",
        "\n",
        "    def bpr_loss(self, pos_edge_rank: Tensor, neg_edge_rank: Tensor,\n",
        "                            lambda_reg: float = 1e-4, **kwargs) -> Tensor:\n",
        "        \"\"\"\n",
        "        Compute the mean Bayesian Personalized Ranking (BPR) loss with optional L2 regularization.\n",
        "\n",
        "        Args:\n",
        "            pos_edge_rank (Tensor): Scores for positive edge pairs.\n",
        "            neg_edge_rank (Tensor): Scores for negative edge pairs.\n",
        "            lambda_reg (float): Regularization strength for L2 regularization (default: 1e-4).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: The computed BPR loss.\n",
        "        \"\"\"\n",
        "        # Compute the pairwise differences between positive and negative rankings\n",
        "        differences = pos_edge_rank - neg_edge_rank\n",
        "        # Compute the BPR loss: log-sigmoid of the differences\n",
        "        loss = -torch.log(torch.sigmoid(differences)).mean()\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77cfe0e9-f76c-48a1-8521-a9fbb9af5658",
      "metadata": {
        "id": "77cfe0e9-f76c-48a1-8521-a9fbb9af5658"
      },
      "outputs": [],
      "source": [
        "def sample_negative_edges(data, num_playlists, num_tracks):\n",
        "    playlists = data.edge_label_index[0, :]  # Source nodes (playlists)\n",
        "    tracks = torch.randint(\n",
        "        num_playlists,\n",
        "        num_playlists + num_tracks - 1,\n",
        "        size=data.edge_label_index[1, :].size()\n",
        "    )  # Random track indices\n",
        "\n",
        "    # Stack the playlists and tracks to form negative edge indices\n",
        "    neg_edge_index = torch.stack((playlists, tracks), dim=0)\n",
        "    neg_edge_label = torch.zeros(neg_edge_index.shape[1])  # All-zero labels for negative edges\n",
        "\n",
        "    return neg_edge_index, neg_edge_label\n",
        "\n",
        "\n",
        "def sample_hard_negative_edges(data, model, num_playlists, num_tracks, device=None, batch_size=500, frac_sample = 1):\n",
        "    with torch.no_grad():\n",
        "        embeddings = model.get_embedding(data.edge_index)\n",
        "        playlists_embeddings = embeddings[:num_playlists].to(device)\n",
        "        tracks_embeddings = embeddings[num_playlists:].to(device)\n",
        "\n",
        "    positive_playlists, positive_tracks = data.edge_label_index\n",
        "    num_edges = positive_playlists.size(0)\n",
        "\n",
        "    # Create a boolean mask for all the positive edges\n",
        "    positive_mask = torch.zeros(num_playlists, num_tracks, device=device, dtype=torch.bool)\n",
        "    positive_mask[positive_playlists, positive_tracks - num_playlists] = True\n",
        "\n",
        "    neg_edges_list = []\n",
        "    neg_edge_label_list = []\n",
        "\n",
        "    for batch_start in range(0, num_edges, batch_size):\n",
        "        batch_end = min(batch_start + batch_size, num_edges)\n",
        "\n",
        "        batch_scores = torch.matmul(\n",
        "            playlists_embeddings[positive_playlists[batch_start:batch_end]], tracks_embeddings.t()\n",
        "        )\n",
        "\n",
        "        # Set the scores of the positive edges to negative infinity\n",
        "        batch_scores[positive_mask[positive_playlists[batch_start:batch_end]]] = -float(\"inf\")\n",
        "\n",
        "        # Select the top k highest scoring negative edges for each playlist in the current batch\n",
        "        # do 0.99 to filter out all pos edges which will be at the end\n",
        "        _, top_indices = torch.topk(batch_scores, int(frac_sample * 0.99 * num_tracks), dim=1)\n",
        "        selected_indices = torch.randint(0, int(frac_sample * 0.99 *num_tracks), size = (batch_end - batch_start, ))\n",
        "        top_indices_selected = top_indices[torch.arange(batch_end - batch_start), selected_indices] + n_playlists\n",
        "\n",
        "        # Create the negative edges tensor for the current batch\n",
        "        neg_edges_batch = torch.stack(\n",
        "            (positive_playlists[batch_start:batch_end], top_indices_selected), dim=0\n",
        "        )\n",
        "        neg_edge_label_batch = torch.zeros(neg_edges_batch.shape[1], device=device)\n",
        "\n",
        "        neg_edges_list.append(neg_edges_batch)\n",
        "        neg_edge_label_list.append(neg_edge_label_batch)\n",
        "\n",
        "    # Concatenate the batch tensors\n",
        "    neg_edges = torch.cat(neg_edges_list, dim=1)\n",
        "    neg_edge_label = torch.cat(neg_edge_label_list)\n",
        "\n",
        "    return neg_edges, neg_edge_label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6fbb6a0-7294-4b6f-a942-8963c2d2d48d",
      "metadata": {
        "id": "b6fbb6a0-7294-4b6f-a942-8963c2d2d48d"
      },
      "outputs": [],
      "source": [
        "def metrics(labels, preds):\n",
        "  roc = roc_auc_score(labels.flatten().cpu().numpy(), preds.flatten().data.cpu().numpy())\n",
        "  return roc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5493fab5-ed7d-4f80-9216-2b16ca3aa223",
      "metadata": {
        "id": "5493fab5-ed7d-4f80-9216-2b16ca3aa223"
      },
      "outputs": [],
      "source": [
        "# Train\n",
        "def train(datasets, model, optimizer, args):\n",
        "  print(f\"Beginning training\")\n",
        "\n",
        "  train_data = datasets[\"train\"]\n",
        "  val_data = datasets[\"val\"]\n",
        "\n",
        "  stats = {\n",
        "      'train': {\n",
        "        'loss': [],\n",
        "        'roc' : []\n",
        "      },\n",
        "      'val': {\n",
        "        'loss': [],\n",
        "        'recall': [],\n",
        "        'precision': [],\n",
        "        'roc' : []\n",
        "      }\n",
        "\n",
        "  }\n",
        "  val_neg_edge, val_neg_label = None, None\n",
        "  for epoch in range(args[\"epochs\"]): # loop over each epoch\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    neg_edge_index, neg_edge_label = sample_negative_edges(train_data, n_playlists, n_tracks)\n",
        "\n",
        "    # calculate embedding\n",
        "    embed = model.get_embedding(train_data.edge_index)\n",
        "    # calculate pos, negative scores using embedding\n",
        "    pos_scores = model.predict_link(embed, train_data.edge_label_index)\n",
        "    neg_scores = model.predict_link(embed, neg_edge_index)\n",
        "\n",
        "    # concatenate pos, neg scores together and evaluate loss\n",
        "    scores = torch.cat((pos_scores, neg_scores), dim = 0)\n",
        "    labels = torch.cat((train_data.edge_label, neg_edge_label), dim = 0)\n",
        "\n",
        "    loss = model.bpr_loss(pos_scores, neg_scores, lambda_reg = 0)\n",
        "\n",
        "    train_roc = metrics(labels, scores)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    val_loss, val_roc, val_neg_edge, val_neg_label = test(model, val_data, epoch, val_neg_edge, val_neg_label)\n",
        "\n",
        "    stats['train']['loss'].append(loss)\n",
        "    stats['train']['roc'].append(train_roc)\n",
        "    stats['val']['loss'].append(val_loss)\n",
        "    stats['val']['roc'].append(val_roc)\n",
        "\n",
        "    print(f\"Epoch {epoch}; Train loss {loss}; Val loss {val_loss}; Train ROC {train_roc}; Val ROC {val_roc}\")\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "      # calculate recall @ K\n",
        "      val_recall = recall_at_k(val_data, model, k = 300, device = args[\"device\"])\n",
        "      val_precision = precision_at_k(val_data, model, k = 300, device = args[\"device\"])\n",
        "      print(f\"Val recall {val_recall}\")\n",
        "      print(f\"Precision recall {val_precision}\")\n",
        "      stats['val']['recall'].append(val_recall)\n",
        "      stats['val']['precision'].append(val_precision)\n",
        "\n",
        "    if epoch % 20 == 0:\n",
        "      path = os.path.join(\"model_embeddings\", f\"{model.conv_layer}_RUN5\")\n",
        "      if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "      torch.save(model.embedding.weight, os.path.join(\"model_embeddings\", f\"{model.conv_layer}_RUN5.pt\"))\n",
        "\n",
        "  pickle.dump(stats, open(f\"model_stats/{model.conv_layer}_RUN5.pkl\", \"wb\"))\n",
        "  return stats\n",
        "\n",
        "def test(model, data, epoch = 0, neg_edge_index = None, neg_edge_label = None):\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad(): # want to save RAM\n",
        "\n",
        "    # conduct negative sampling\n",
        "    neg_edge_index, neg_edge_label = sample_negative_edges(data, n_playlists, n_tracks)\n",
        "    # obtain model embedding\n",
        "    embed = model.get_embedding(data.edge_index)\n",
        "    # calculate pos, neg scores using embedding\n",
        "    pos_scores = model.predict_link(embed, data.edge_label_index)\n",
        "    neg_scores = model.predict_link(embed, neg_edge_index)\n",
        "    # concatenate pos, neg scores together and evaluate loss\n",
        "    scores = torch.cat((pos_scores, neg_scores), dim = 0)\n",
        "    labels = torch.cat((data.edge_label, neg_edge_label), dim = 0)\n",
        "    # calculate loss\n",
        "    loss = model.bpr_loss(pos_scores, neg_scores, lambda_reg = 0)\n",
        "\n",
        "    roc = metrics(labels, scores)\n",
        "\n",
        "  return loss, roc, neg_edge_index, neg_edge_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caeef716-8fc7-48c5-92ca-98684111c7f1",
      "metadata": {
        "id": "caeef716-8fc7-48c5-92ca-98684111c7f1"
      },
      "outputs": [],
      "source": [
        "datasets = {\n",
        "    'train':train_split,\n",
        "    'val':val_split,\n",
        "    'test': test_split\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f00f4a1c-c8f6-4403-9e4d-cfea8881117a",
      "metadata": {
        "id": "f00f4a1c-c8f6-4403-9e4d-cfea8881117a"
      },
      "outputs": [],
      "source": [
        "# initialize our arguments\n",
        "args = {\n",
        "    'device' : 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "    'num_layers' :  3,\n",
        "    'emb_size' : 32,\n",
        "    'weight_decay': 1e-5,\n",
        "    'lr': 0.01,\n",
        "    'epochs': 100\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c190eab3-1e51-4921-83c4-eb32fa15ccdb",
      "metadata": {
        "id": "c190eab3-1e51-4921-83c4-eb32fa15ccdb"
      },
      "outputs": [],
      "source": [
        "num_nodes = n_playlists + n_tracks\n",
        "model = GCN(\n",
        "    num_nodes = num_nodes, num_layers = args['num_layers'],\n",
        "    embedding_dim = args[\"emb_size\"], conv_layer = \"LGC\"\n",
        ")\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "691b8cb2-0761-4d5d-bc09-4d9c0432f978",
      "metadata": {
        "id": "691b8cb2-0761-4d5d-bc09-4d9c0432f978",
        "outputId": "d9324e2f-3d07-449a-e931-272164770e7d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GCN(\n",
              "  (embedding): Embedding(570402, 32)\n",
              "  (convs): ModuleList(\n",
              "    (0-2): 3 x LGConv()\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# send data, model to GPU if available\n",
        "playlists_idx = torch.Tensor(playlists_idx).type(torch.int64).to(args[\"device\"])\n",
        "tracks_idx =torch.Tensor(tracks_idx).type(torch.int64).to(args[\"device\"])\n",
        "datasets['train'].to(args['device'])\n",
        "datasets['val'].to(args['device'])\n",
        "datasets['test'].to(args['device'])\n",
        "model.to(args[\"device\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77650859-72e1-4615-9e7b-8aecfc84eefc",
      "metadata": {
        "id": "77650859-72e1-4615-9e7b-8aecfc84eefc"
      },
      "outputs": [],
      "source": [
        "MODEL_STATS_DIR = \"model_stats\"\n",
        "if not os.path.exists(MODEL_STATS_DIR):\n",
        "  os.makedirs(MODEL_STATS_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74d4ef83-a380-4e02-8453-a2115d18d12f",
      "metadata": {
        "id": "74d4ef83-a380-4e02-8453-a2115d18d12f",
        "outputId": "9803986c-a087-430e-b566-c734356c2685"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Beginning training\n",
            "Epoch 0; Train loss 0.6931471228599548; Val loss 0.6931471824645996; Train ROC 0.5000410526797249; Val ROC 0.5001638376802623\n",
            "Val recall 0.13609260320663452\n",
            "Precision recall 3.1451432732865214e-05\n",
            "Epoch 1; Train loss 0.6931471228599548; Val loss 0.6931471824645996; Train ROC 0.4993342313961204; Val ROC 0.4994723899344939\n",
            "Epoch 2; Train loss 0.6931471228599548; Val loss 0.6931471824645996; Train ROC 0.5001680541662287; Val ROC 0.5004098472102604\n",
            "Epoch 3; Train loss 0.6931471228599548; Val loss 0.6931471824645996; Train ROC 0.4986793680108323; Val ROC 0.5002261462992315\n",
            "Epoch 4; Train loss 0.6931471228599548; Val loss 0.6931471824645996; Train ROC 0.49905440908058807; Val ROC 0.49997373700067016\n",
            "Epoch 5; Train loss 0.6931471228599548; Val loss 0.6931471824645996; Train ROC 0.4997986302798773; Val ROC 0.49990620406662445\n",
            "Epoch 6; Train loss 0.6931471228599548; Val loss 0.6931471824645996; Train ROC 0.4995683701058386; Val ROC 0.500327197657991\n",
            "Epoch 7; Train loss 0.6931471228599548; Val loss 0.6931471824645996; Train ROC 0.49856270243106154; Val ROC 0.5002637911944984\n",
            "Epoch 8; Train loss 0.6931471228599548; Val loss 0.6931471824645996; Train ROC 0.49886593729418316; Val ROC 0.5000507599157584\n",
            "Epoch 9; Train loss 0.6931471228599548; Val loss 0.6931471824645996; Train ROC 0.49946181862864636; Val ROC 0.4997698359509288\n",
            "Epoch 10; Train loss 0.6931471228599548; Val loss 0.6931471824645996; Train ROC 0.4998255820699173; Val ROC 0.500325144285055\n",
            "Val recall 0.13613493740558624\n",
            "Precision recall 3.216664481442422e-05\n",
            "Epoch 11; Train loss 0.6931471228599548; Val loss 0.6931471824645996; Train ROC 0.4985298673460088; Val ROC 0.500401875180621\n",
            "Epoch 12; Train loss 0.6931471228599548; Val loss 0.6931471824645996; Train ROC 0.4983783443566883; Val ROC 0.500196480794533\n",
            "Epoch 13; Train loss 0.6931471228599548; Val loss 0.6931471824645996; Train ROC 0.4989820648856289; Val ROC 0.4996522860992722\n",
            "Epoch 14; Train loss 0.6931471228599548; Val loss 0.6931471824645996; Train ROC 0.49989572291426226; Val ROC 0.5001366808777887\n",
            "Epoch 15; Train loss 0.6931471228599548; Val loss 0.6931471824645996; Train ROC 0.4993125773079208; Val ROC 0.5004347548647889\n",
            "Epoch 16; Train loss 0.6931471228599548; Val loss 0.6931471824645996; Train ROC 0.4980938354547989; Val ROC 0.5002003564444581\n",
            "Epoch 17; Train loss 0.6931471228599548; Val loss 0.6931471824645996; Train ROC 0.49833195273989644; Val ROC 0.4998783110186337\n",
            "Epoch 18; Train loss 0.6931471228599548; Val loss 0.6931471824645996; Train ROC 0.499011958564818; Val ROC 0.4998126740040119\n",
            "Epoch 19; Train loss 0.6931471228599548; Val loss 0.6931471824645996; Train ROC 0.4997352096739272; Val ROC 0.5003451501421704\n",
            "Epoch 20; Train loss 0.6931471824645996; Val loss 0.6931471824645996; Train ROC 0.4986278749529027; Val ROC 0.5004818113200937\n",
            "Val recall 0.13609570264816284\n",
            "Precision recall 3.1415667763212696e-05\n",
            "Epoch 21; Train loss 0.6931471228599548; Val loss 0.6931471824645996; Train ROC 0.4976222335751536; Val ROC 0.5000304967462663\n",
            "Epoch 22; Train loss 0.6931471228599548; Val loss 0.6931471824645996; Train ROC 0.49809944747913515; Val ROC 0.5000131408475663\n",
            "Epoch 23; Train loss 0.6931471228599548; Val loss 0.6931471824645996; Train ROC 0.49909195586575794; Val ROC 0.4997872789775464\n",
            "Epoch 24; Train loss 0.6931471824645996; Val loss 0.6931471824645996; Train ROC 0.4992310581615886; Val ROC 0.5002389665147398\n",
            "Epoch 25; Train loss 0.6931472420692444; Val loss 0.6931471824645996; Train ROC 0.49791685173374556; Val ROC 0.5005587590931083\n",
            "Epoch 26; Train loss 0.6931471228599548; Val loss 0.6931471824645996; Train ROC 0.49759784271827734; Val ROC 0.4999848687546937\n",
            "Epoch 27; Train loss 0.6931471228599548; Val loss 0.6931471824645996; Train ROC 0.4981575432744074; Val ROC 0.4996521132288878\n",
            "Epoch 28; Train loss 0.6931471228599548; Val loss 0.6931471824645996; Train ROC 0.49889873194689166; Val ROC 0.5000270541325083\n",
            "Epoch 29; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.49870331958120795; Val ROC 0.500543254103895\n",
            "Epoch 30; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.4971370147592862; Val ROC 0.5002776542503914\n",
            "Val recall 0.1360863596200943\n",
            "Precision recall 3.185373861924745e-05\n",
            "Epoch 31; Train loss 0.6931471824645996; Val loss 0.6931471824645996; Train ROC 0.4971808304370808; Val ROC 0.49996342511259156\n",
            "Epoch 32; Train loss 0.6931471824645996; Val loss 0.6931471824645996; Train ROC 0.49814742784324556; Val ROC 0.49984313798660945\n",
            "Epoch 33; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.49888247451048495; Val ROC 0.5000840258113347\n",
            "Epoch 34; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.49790396087569205; Val ROC 0.5007744817505734\n",
            "Epoch 35; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.4963698478207721; Val ROC 0.5001430832999533\n",
            "Epoch 36; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.4973878881670107; Val ROC 0.4999386267204124\n",
            "Epoch 37; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.49830483404840104; Val ROC 0.5001697637422144\n",
            "Epoch 38; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.4987761333120468; Val ROC 0.5001467916895173\n",
            "Epoch 39; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.49706512730125774; Val ROC 0.5006339663951985\n",
            "Epoch 40; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.4965486651558231; Val ROC 0.5002206585935589\n",
            "Val recall 0.13608811795711517\n",
            "Precision recall 3.188056143699214e-05\n",
            "Epoch 41; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.4974144070681412; Val ROC 0.4999127387312164\n",
            "Epoch 42; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.4983139495645699; Val ROC 0.5002211697397594\n",
            "Epoch 43; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.49836765408007555; Val ROC 0.50048888610558\n",
            "Epoch 44; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.49633161968793255; Val ROC 0.5001182391778234\n",
            "Epoch 45; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.49664282759925044; Val ROC 0.5002133149851911\n",
            "Epoch 46; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.49786866175430006; Val ROC 0.4998560302647908\n",
            "Epoch 47; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.49847353105455805; Val ROC 0.5004901250834072\n",
            "Epoch 48; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.49706778968085563; Val ROC 0.5008112214540704\n",
            "Epoch 49; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.49587531300466603; Val ROC 0.49968373871367494\n",
            "Epoch 50; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.4969591656629918; Val ROC 0.4998846898600224\n",
            "Val recall 0.13616225123405457\n",
            "Precision recall 3.1639177905162796e-05\n",
            "Epoch 51; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.49801921244806424; Val ROC 0.5000428654809808\n",
            "Epoch 52; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.49822569499099933; Val ROC 0.5007290249312234\n",
            "Epoch 53; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.4959709783688739; Val ROC 0.5002458338942981\n",
            "Epoch 54; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.4960524938643993; Val ROC 0.49974991275010333\n",
            "Epoch 55; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.49747038365942764; Val ROC 0.49999159624675527\n",
            "Epoch 56; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.498448948399123; Val ROC 0.5006032249366497\n",
            "Epoch 57; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.49688658040799333; Val ROC 0.5001992177925926\n",
            "Epoch 58; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.4955941914989402; Val ROC 0.499832804875402\n",
            "Epoch 59; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.496804027504033; Val ROC 0.4997884270739194\n",
            "Epoch 60; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.4981152405245566; Val ROC 0.5001525871739639\n",
            "Val recall 0.13611412048339844\n",
            "Precision recall 3.2425909012090415e-05\n",
            "Epoch 61; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.4982175508454139; Val ROC 0.5000930493799396\n",
            "Epoch 62; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.4953436109153396; Val ROC 0.5001819552406805\n",
            "Epoch 63; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.49582993150909516; Val ROC 0.4998996422053485\n",
            "Epoch 64; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.49767040253554434; Val ROC 0.5000228069467899\n",
            "Epoch 65; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.4985117263516037; Val ROC 0.5003232915210049\n",
            "Epoch 66; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.4961526422153944; Val ROC 0.5001843005501769\n",
            "Epoch 67; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.4952244572727703; Val ROC 0.49989419159596954\n",
            "Epoch 68; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.4970678675773824; Val ROC 0.500027355410835\n",
            "Epoch 69; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.4984414097631744; Val ROC 0.49990269862876024\n",
            "Epoch 70; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.49713285063638973; Val ROC 0.5004360142097507\n",
            "Val recall 0.1362324208021164\n",
            "Precision recall 3.365070733707398e-05\n",
            "Epoch 71; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.4947297062626623; Val ROC 0.5001240785359179\n",
            "Epoch 72; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.4958410006800883; Val ROC 0.49985348609410907\n",
            "Epoch 73; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.4981865285262961; Val ROC 0.5000734360916473\n",
            "Epoch 74; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.497901475433492; Val ROC 0.5004253689310131\n",
            "Epoch 75; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.49463792321375877; Val ROC 0.4998924192879708\n",
            "Epoch 76; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.4953467994545634; Val ROC 0.4995724183909979\n",
            "Epoch 77; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.4976104705507052; Val ROC 0.499852112956841\n",
            "Epoch 78; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.498563782432601; Val ROC 0.5004227324592704\n",
            "Epoch 79; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.49544737807404077; Val ROC 0.5003222552015791\n",
            "Epoch 80; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.4948186555892016; Val ROC 0.4999188994438544\n",
            "Val recall 0.1361197531223297\n",
            "Precision recall 3.144249421893619e-05\n",
            "Epoch 81; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.4968120870377123; Val ROC 0.5000311534354053\n",
            "Epoch 82; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.49853211586302354; Val ROC 0.5003248831227183\n",
            "Epoch 83; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.49622100213747494; Val ROC 0.5001386046345186\n",
            "Epoch 84; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.49420190606951214; Val ROC 0.4998102520376945\n",
            "Epoch 85; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.49633503484946906; Val ROC 0.4998269298093956\n",
            "Epoch 86; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.49825811977645634; Val ROC 0.5004805128239206\n",
            "Epoch 87; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.497131974190998; Val ROC 0.5005658607968151\n",
            "Epoch 88; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.49397874876351555; Val ROC 0.49989649319114965\n",
            "Epoch 89; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.495907064682174; Val ROC 0.5000387618193618\n",
            "Epoch 90; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.49817429782652217; Val ROC 0.500400094719981\n",
            "Val recall 0.13617008924484253\n",
            "Precision recall 3.289079904789105e-05\n",
            "Epoch 91; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.4977232441508743; Val ROC 0.5005268598224488\n",
            "Epoch 92; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.494041793304163; Val ROC 0.5001522553944744\n",
            "Epoch 93; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.49532053295559636; Val ROC 0.49981039418037226\n",
            "Epoch 94; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.49801636453601866; Val ROC 0.5001875543359596\n",
            "Epoch 95; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.4981580590551407; Val ROC 0.500697558217729\n",
            "Epoch 96; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.49434838980088835; Val ROC 0.5002822478803169\n",
            "Epoch 97; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.49474980971829036; Val ROC 0.4999421695252502\n",
            "Epoch 98; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.4976004687235657; Val ROC 0.500027978141044\n",
            "Epoch 99; Train loss 0.6931475400924683; Val loss 0.6931471824645996; Train ROC 0.49862509952300593; Val ROC 0.5003486784461088\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'train': {'loss': [tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>),\n",
              "   tensor(0.6931, grad_fn=<NegBackward0>)],\n",
              "  'roc': [np.float64(0.5000410526797249),\n",
              "   np.float64(0.4993342313961204),\n",
              "   np.float64(0.5001680541662287),\n",
              "   np.float64(0.4986793680108323),\n",
              "   np.float64(0.49905440908058807),\n",
              "   np.float64(0.4997986302798773),\n",
              "   np.float64(0.4995683701058386),\n",
              "   np.float64(0.49856270243106154),\n",
              "   np.float64(0.49886593729418316),\n",
              "   np.float64(0.49946181862864636),\n",
              "   np.float64(0.4998255820699173),\n",
              "   np.float64(0.4985298673460088),\n",
              "   np.float64(0.4983783443566883),\n",
              "   np.float64(0.4989820648856289),\n",
              "   np.float64(0.49989572291426226),\n",
              "   np.float64(0.4993125773079208),\n",
              "   np.float64(0.4980938354547989),\n",
              "   np.float64(0.49833195273989644),\n",
              "   np.float64(0.499011958564818),\n",
              "   np.float64(0.4997352096739272),\n",
              "   np.float64(0.4986278749529027),\n",
              "   np.float64(0.4976222335751536),\n",
              "   np.float64(0.49809944747913515),\n",
              "   np.float64(0.49909195586575794),\n",
              "   np.float64(0.4992310581615886),\n",
              "   np.float64(0.49791685173374556),\n",
              "   np.float64(0.49759784271827734),\n",
              "   np.float64(0.4981575432744074),\n",
              "   np.float64(0.49889873194689166),\n",
              "   np.float64(0.49870331958120795),\n",
              "   np.float64(0.4971370147592862),\n",
              "   np.float64(0.4971808304370808),\n",
              "   np.float64(0.49814742784324556),\n",
              "   np.float64(0.49888247451048495),\n",
              "   np.float64(0.49790396087569205),\n",
              "   np.float64(0.4963698478207721),\n",
              "   np.float64(0.4973878881670107),\n",
              "   np.float64(0.49830483404840104),\n",
              "   np.float64(0.4987761333120468),\n",
              "   np.float64(0.49706512730125774),\n",
              "   np.float64(0.4965486651558231),\n",
              "   np.float64(0.4974144070681412),\n",
              "   np.float64(0.4983139495645699),\n",
              "   np.float64(0.49836765408007555),\n",
              "   np.float64(0.49633161968793255),\n",
              "   np.float64(0.49664282759925044),\n",
              "   np.float64(0.49786866175430006),\n",
              "   np.float64(0.49847353105455805),\n",
              "   np.float64(0.49706778968085563),\n",
              "   np.float64(0.49587531300466603),\n",
              "   np.float64(0.4969591656629918),\n",
              "   np.float64(0.49801921244806424),\n",
              "   np.float64(0.49822569499099933),\n",
              "   np.float64(0.4959709783688739),\n",
              "   np.float64(0.4960524938643993),\n",
              "   np.float64(0.49747038365942764),\n",
              "   np.float64(0.498448948399123),\n",
              "   np.float64(0.49688658040799333),\n",
              "   np.float64(0.4955941914989402),\n",
              "   np.float64(0.496804027504033),\n",
              "   np.float64(0.4981152405245566),\n",
              "   np.float64(0.4982175508454139),\n",
              "   np.float64(0.4953436109153396),\n",
              "   np.float64(0.49582993150909516),\n",
              "   np.float64(0.49767040253554434),\n",
              "   np.float64(0.4985117263516037),\n",
              "   np.float64(0.4961526422153944),\n",
              "   np.float64(0.4952244572727703),\n",
              "   np.float64(0.4970678675773824),\n",
              "   np.float64(0.4984414097631744),\n",
              "   np.float64(0.49713285063638973),\n",
              "   np.float64(0.4947297062626623),\n",
              "   np.float64(0.4958410006800883),\n",
              "   np.float64(0.4981865285262961),\n",
              "   np.float64(0.497901475433492),\n",
              "   np.float64(0.49463792321375877),\n",
              "   np.float64(0.4953467994545634),\n",
              "   np.float64(0.4976104705507052),\n",
              "   np.float64(0.498563782432601),\n",
              "   np.float64(0.49544737807404077),\n",
              "   np.float64(0.4948186555892016),\n",
              "   np.float64(0.4968120870377123),\n",
              "   np.float64(0.49853211586302354),\n",
              "   np.float64(0.49622100213747494),\n",
              "   np.float64(0.49420190606951214),\n",
              "   np.float64(0.49633503484946906),\n",
              "   np.float64(0.49825811977645634),\n",
              "   np.float64(0.497131974190998),\n",
              "   np.float64(0.49397874876351555),\n",
              "   np.float64(0.495907064682174),\n",
              "   np.float64(0.49817429782652217),\n",
              "   np.float64(0.4977232441508743),\n",
              "   np.float64(0.494041793304163),\n",
              "   np.float64(0.49532053295559636),\n",
              "   np.float64(0.49801636453601866),\n",
              "   np.float64(0.4981580590551407),\n",
              "   np.float64(0.49434838980088835),\n",
              "   np.float64(0.49474980971829036),\n",
              "   np.float64(0.4976004687235657),\n",
              "   np.float64(0.49862509952300593)]},\n",
              " 'val': {'loss': [tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931),\n",
              "   tensor(0.6931)],\n",
              "  'recall': [0.13609260320663452,\n",
              "   0.13613493740558624,\n",
              "   0.13609570264816284,\n",
              "   0.1360863596200943,\n",
              "   0.13608811795711517,\n",
              "   0.13616225123405457,\n",
              "   0.13611412048339844,\n",
              "   0.1362324208021164,\n",
              "   0.1361197531223297,\n",
              "   0.13617008924484253],\n",
              "  'precision': [3.1451432732865214e-05,\n",
              "   3.216664481442422e-05,\n",
              "   3.1415667763212696e-05,\n",
              "   3.185373861924745e-05,\n",
              "   3.188056143699214e-05,\n",
              "   3.1639177905162796e-05,\n",
              "   3.2425909012090415e-05,\n",
              "   3.365070733707398e-05,\n",
              "   3.144249421893619e-05,\n",
              "   3.289079904789105e-05],\n",
              "  'roc': [np.float64(0.5001638376802623),\n",
              "   np.float64(0.4994723899344939),\n",
              "   np.float64(0.5004098472102604),\n",
              "   np.float64(0.5002261462992315),\n",
              "   np.float64(0.49997373700067016),\n",
              "   np.float64(0.49990620406662445),\n",
              "   np.float64(0.500327197657991),\n",
              "   np.float64(0.5002637911944984),\n",
              "   np.float64(0.5000507599157584),\n",
              "   np.float64(0.4997698359509288),\n",
              "   np.float64(0.500325144285055),\n",
              "   np.float64(0.500401875180621),\n",
              "   np.float64(0.500196480794533),\n",
              "   np.float64(0.4996522860992722),\n",
              "   np.float64(0.5001366808777887),\n",
              "   np.float64(0.5004347548647889),\n",
              "   np.float64(0.5002003564444581),\n",
              "   np.float64(0.4998783110186337),\n",
              "   np.float64(0.4998126740040119),\n",
              "   np.float64(0.5003451501421704),\n",
              "   np.float64(0.5004818113200937),\n",
              "   np.float64(0.5000304967462663),\n",
              "   np.float64(0.5000131408475663),\n",
              "   np.float64(0.4997872789775464),\n",
              "   np.float64(0.5002389665147398),\n",
              "   np.float64(0.5005587590931083),\n",
              "   np.float64(0.4999848687546937),\n",
              "   np.float64(0.4996521132288878),\n",
              "   np.float64(0.5000270541325083),\n",
              "   np.float64(0.500543254103895),\n",
              "   np.float64(0.5002776542503914),\n",
              "   np.float64(0.49996342511259156),\n",
              "   np.float64(0.49984313798660945),\n",
              "   np.float64(0.5000840258113347),\n",
              "   np.float64(0.5007744817505734),\n",
              "   np.float64(0.5001430832999533),\n",
              "   np.float64(0.4999386267204124),\n",
              "   np.float64(0.5001697637422144),\n",
              "   np.float64(0.5001467916895173),\n",
              "   np.float64(0.5006339663951985),\n",
              "   np.float64(0.5002206585935589),\n",
              "   np.float64(0.4999127387312164),\n",
              "   np.float64(0.5002211697397594),\n",
              "   np.float64(0.50048888610558),\n",
              "   np.float64(0.5001182391778234),\n",
              "   np.float64(0.5002133149851911),\n",
              "   np.float64(0.4998560302647908),\n",
              "   np.float64(0.5004901250834072),\n",
              "   np.float64(0.5008112214540704),\n",
              "   np.float64(0.49968373871367494),\n",
              "   np.float64(0.4998846898600224),\n",
              "   np.float64(0.5000428654809808),\n",
              "   np.float64(0.5007290249312234),\n",
              "   np.float64(0.5002458338942981),\n",
              "   np.float64(0.49974991275010333),\n",
              "   np.float64(0.49999159624675527),\n",
              "   np.float64(0.5006032249366497),\n",
              "   np.float64(0.5001992177925926),\n",
              "   np.float64(0.499832804875402),\n",
              "   np.float64(0.4997884270739194),\n",
              "   np.float64(0.5001525871739639),\n",
              "   np.float64(0.5000930493799396),\n",
              "   np.float64(0.5001819552406805),\n",
              "   np.float64(0.4998996422053485),\n",
              "   np.float64(0.5000228069467899),\n",
              "   np.float64(0.5003232915210049),\n",
              "   np.float64(0.5001843005501769),\n",
              "   np.float64(0.49989419159596954),\n",
              "   np.float64(0.500027355410835),\n",
              "   np.float64(0.49990269862876024),\n",
              "   np.float64(0.5004360142097507),\n",
              "   np.float64(0.5001240785359179),\n",
              "   np.float64(0.49985348609410907),\n",
              "   np.float64(0.5000734360916473),\n",
              "   np.float64(0.5004253689310131),\n",
              "   np.float64(0.4998924192879708),\n",
              "   np.float64(0.4995724183909979),\n",
              "   np.float64(0.499852112956841),\n",
              "   np.float64(0.5004227324592704),\n",
              "   np.float64(0.5003222552015791),\n",
              "   np.float64(0.4999188994438544),\n",
              "   np.float64(0.5000311534354053),\n",
              "   np.float64(0.5003248831227183),\n",
              "   np.float64(0.5001386046345186),\n",
              "   np.float64(0.4998102520376945),\n",
              "   np.float64(0.4998269298093956),\n",
              "   np.float64(0.5004805128239206),\n",
              "   np.float64(0.5005658607968151),\n",
              "   np.float64(0.49989649319114965),\n",
              "   np.float64(0.5000387618193618),\n",
              "   np.float64(0.500400094719981),\n",
              "   np.float64(0.5005268598224488),\n",
              "   np.float64(0.5001522553944744),\n",
              "   np.float64(0.49981039418037226),\n",
              "   np.float64(0.5001875543359596),\n",
              "   np.float64(0.500697558217729),\n",
              "   np.float64(0.5002822478803169),\n",
              "   np.float64(0.4999421695252502),\n",
              "   np.float64(0.500027978141044),\n",
              "   np.float64(0.5003486784461088)]}}"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train(datasets, model, optimizer, args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "621e90a5-c380-42dc-921b-626a633b38c4",
      "metadata": {
        "id": "621e90a5-c380-42dc-921b-626a633b38c4",
        "outputId": "8d22c199-3cbe-437f-852e-76bac34e3ef2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor(0.6931),\n",
              " np.float64(0.5003972918357218),\n",
              " tensor([[ 34388, 297775, 358124,  ...,  93364, 348922, 182526],\n",
              "         [387086, 437016, 379331,  ..., 397298, 404122, 407661]]),\n",
              " tensor([0., 0., 0.,  ..., 0., 0., 0.]))"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test(model, datasets['test'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52c3d829-71d0-427c-988c-5a28eb363072",
      "metadata": {
        "id": "52c3d829-71d0-427c-988c-5a28eb363072",
        "outputId": "bce9ecfd-2d3d-43a8-f7f5-7a1a12ffc4c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Beginning training\n",
            "Epoch 0; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.49957943426104445; Val ROC 0.5001666637654862\n",
            "Val recall 0.13975803554058075\n",
            "Precision recall 5.032699118601158e-05\n",
            "Epoch 1; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49993227056143147; Val ROC 0.4999376222242458\n",
            "Epoch 2; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4997003067892545; Val ROC 0.5003021064833647\n",
            "Epoch 3; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.4997446046995841; Val ROC 0.500434413562972\n",
            "Epoch 4; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4998322381733959; Val ROC 0.5002221915701812\n",
            "Epoch 5; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.49967846961772727; Val ROC 0.5000080551701347\n",
            "Epoch 6; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4998306081413386; Val ROC 0.4994999756649391\n",
            "Epoch 7; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4996862155391929; Val ROC 0.5005810912071302\n",
            "Epoch 8; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49985857893038593; Val ROC 0.5002193692282618\n",
            "Epoch 9; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49938720803423425; Val ROC 0.4997548179704598\n",
            "Epoch 10; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49963864285942594; Val ROC 0.5003744760080967\n",
            "Val recall 0.13975803554058075\n",
            "Precision recall 5.032699118601158e-05\n",
            "Epoch 11; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.49993951611346227; Val ROC 0.5001888877427978\n",
            "Epoch 12; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.5000205191383406; Val ROC 0.4998842042907746\n",
            "Epoch 13; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49980309171336; Val ROC 0.5002955099940285\n",
            "Epoch 14; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4997248890120915; Val ROC 0.4998296679231949\n",
            "Epoch 15; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.4996464697626125; Val ROC 0.5004065864398016\n",
            "Epoch 16; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4996346807496453; Val ROC 0.5006735165118886\n",
            "Epoch 17; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.49980213354192626; Val ROC 0.4998060388123952\n",
            "Epoch 18; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4997699025586339; Val ROC 0.5002558089927365\n",
            "Epoch 19; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4995514474399406; Val ROC 0.5002355465916035\n",
            "Epoch 20; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.49984389617648456; Val ROC 0.49985317008048075\n",
            "Val recall 0.13975803554058075\n",
            "Precision recall 5.032699118601158e-05\n",
            "Epoch 21; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49983718284897877; Val ROC 0.49970665223792066\n",
            "Epoch 22; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49985724551795907; Val ROC 0.5002673349303391\n",
            "Epoch 23; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49965693207613154; Val ROC 0.5006728282476032\n",
            "Epoch 24; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49977162913115303; Val ROC 0.5000705129522236\n",
            "Epoch 25; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49952922928101584; Val ROC 0.5002181258473989\n",
            "Epoch 26; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.4996715413586587; Val ROC 0.4997824081889174\n",
            "Epoch 27; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4997830172463646; Val ROC 0.4993285607527574\n",
            "Epoch 28; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4998255885853688; Val ROC 0.4999553057226896\n",
            "Epoch 29; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4996348550283682; Val ROC 0.5006788375833612\n",
            "Epoch 30; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49954002087472377; Val ROC 0.4995104038095556\n",
            "Val recall 0.13975803554058075\n",
            "Precision recall 5.032699118601158e-05\n",
            "Epoch 31; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.499659782622627; Val ROC 0.5005708902294836\n",
            "Epoch 32; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4997540350517604; Val ROC 0.5002010721037763\n",
            "Epoch 33; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49997434871090796; Val ROC 0.5007651345179777\n",
            "Epoch 34; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49965746752859597; Val ROC 0.500126826012857\n",
            "Epoch 35; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.499634387266027; Val ROC 0.5002244476569624\n",
            "Epoch 36; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.4997600721875118; Val ROC 0.5002346739438078\n",
            "Epoch 37; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4998412630273567; Val ROC 0.49997753496611397\n",
            "Epoch 38; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.4997598300548478; Val ROC 0.50013302067997\n",
            "Epoch 39; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4997767521748817; Val ROC 0.5007041406092806\n",
            "Epoch 40; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49981190623950844; Val ROC 0.4997890500337012\n",
            "Val recall 0.13975803554058075\n",
            "Precision recall 5.032699118601158e-05\n",
            "Epoch 41; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4998069522197118; Val ROC 0.5000823554518551\n",
            "Epoch 42; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4997372628147911; Val ROC 0.4996942025115839\n",
            "Epoch 43; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.49961995140261223; Val ROC 0.5001872413321778\n",
            "Epoch 44; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49994505892455127; Val ROC 0.500303575771884\n",
            "Epoch 45; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4997339279550849; Val ROC 0.5003351819243865\n",
            "Epoch 46; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49949469625297055; Val ROC 0.5002136208905816\n",
            "Epoch 47; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4999906278612149; Val ROC 0.5002185206410239\n",
            "Epoch 48; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.49983666840335306; Val ROC 0.5001577633777459\n",
            "Epoch 49; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49942476509555006; Val ROC 0.5006955522095653\n",
            "Epoch 50; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4997159043961516; Val ROC 0.5004879385487709\n",
            "Val recall 0.13975803554058075\n",
            "Precision recall 5.032699118601158e-05\n",
            "Epoch 51; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49973960933744654; Val ROC 0.5001365437832619\n",
            "Epoch 52; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49970994561414533; Val ROC 0.5003988000390457\n",
            "Epoch 53; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4998159944842824; Val ROC 0.5001117748205366\n",
            "Epoch 54; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49958373497715836; Val ROC 0.5000475328021607\n",
            "Epoch 55; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.49952545488414346; Val ROC 0.5001283793761714\n",
            "Epoch 56; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49983075154151674; Val ROC 0.5002213744736886\n",
            "Epoch 57; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.4995800346976351; Val ROC 0.5002880253099471\n",
            "Epoch 58; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.4995781593875124; Val ROC 0.5006834184456409\n",
            "Epoch 59; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.4997282774357809; Val ROC 0.5000245156774492\n",
            "Epoch 60; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.4998797157906365; Val ROC 0.49983028053498696\n",
            "Val recall 0.13975803554058075\n",
            "Precision recall 5.032699118601158e-05\n",
            "Epoch 61; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49962554106328616; Val ROC 0.5000497006500318\n",
            "Epoch 62; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.500003964958944; Val ROC 0.5005969690649087\n",
            "Epoch 63; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49969392691134784; Val ROC 0.5003594455189551\n",
            "Epoch 64; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.4996451720501507; Val ROC 0.500202502592709\n",
            "Epoch 65; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.4996518660347681; Val ROC 0.500270561857383\n",
            "Epoch 66; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4998164767965837; Val ROC 0.5009458765742157\n",
            "Epoch 67; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49988241761511276; Val ROC 0.5005588189150234\n",
            "Epoch 68; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49983859272456216; Val ROC 0.500069182154997\n",
            "Epoch 69; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.49961184439833256; Val ROC 0.5001381859040732\n",
            "Epoch 70; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4996734975408295; Val ROC 0.5003119451731713\n",
            "Val recall 0.13975803554058075\n",
            "Precision recall 5.032699118601158e-05\n",
            "Epoch 71; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4995896561848062; Val ROC 0.5003962524140539\n",
            "Epoch 72; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49960866159390366; Val ROC 0.4997967092441798\n",
            "Epoch 73; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.5000442904310864; Val ROC 0.5002312135216049\n",
            "Epoch 74; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.4998613625586049; Val ROC 0.5000873738066332\n",
            "Epoch 75; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.5000211826497526; Val ROC 0.4997980506791672\n",
            "Epoch 76; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49944454877708333; Val ROC 0.5001145803415683\n",
            "Epoch 77; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.4997691047832031; Val ROC 0.499938421455166\n",
            "Epoch 78; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49966126890809315; Val ROC 0.4999479107340685\n",
            "Epoch 79; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.5001000045582861; Val ROC 0.49992805407895197\n",
            "Epoch 80; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4996346338909603; Val ROC 0.499859471301492\n",
            "Val recall 0.13975803554058075\n",
            "Precision recall 5.032699118601158e-05\n",
            "Epoch 81; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.49972781712257786; Val ROC 0.5003456023731977\n",
            "Epoch 82; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49978802891053226; Val ROC 0.4999701778264673\n",
            "Epoch 83; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4996435667131528; Val ROC 0.5000451012087263\n",
            "Epoch 84; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.4997041348546543; Val ROC 0.4997825343238375\n",
            "Epoch 85; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.49959760261435915; Val ROC 0.5005449350237912\n",
            "Epoch 86; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4999394440854325; Val ROC 0.49998027774518605\n",
            "Epoch 87; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.49963851518764324; Val ROC 0.5001723969767787\n",
            "Epoch 88; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.49965995938729024; Val ROC 0.5004289338709528\n",
            "Epoch 89; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4996884849929539; Val ROC 0.5000424952323757\n",
            "Epoch 90; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.5000329846702128; Val ROC 0.5000302594154332\n",
            "Val recall 0.13975803554058075\n",
            "Precision recall 5.032699118601158e-05\n",
            "Epoch 91; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49990665166273124; Val ROC 0.5003986252913005\n",
            "Epoch 92; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49978581657242366; Val ROC 0.500399183675927\n",
            "Epoch 93; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.49975057866213385; Val ROC 0.4998694009630689\n",
            "Epoch 94; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4996505666621625; Val ROC 0.5001397599737286\n",
            "Epoch 95; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.49972238701965543; Val ROC 0.5004266768209117\n",
            "Epoch 96; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4998013314416562; Val ROC 0.5000445418466096\n",
            "Epoch 97; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4996202028020177; Val ROC 0.5002817098016948\n",
            "Epoch 98; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.4998949908188041; Val ROC 0.5001096882956269\n",
            "Epoch 99; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4998435774190431; Val ROC 0.49993487421661154\n",
            "Epoch 100; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.4995511078738087; Val ROC 0.49968293994878643\n",
            "Val recall 0.13975803554058075\n",
            "Precision recall 5.032699118601158e-05\n",
            "Epoch 101; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4997536103713433; Val ROC 0.5010567612185448\n",
            "Epoch 102; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49972812158896573; Val ROC 0.5003205575832455\n",
            "Epoch 103; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49986635058944157; Val ROC 0.5003095719686453\n",
            "Epoch 104; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49955348748410205; Val ROC 0.499636924918323\n",
            "Epoch 105; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49991918969585314; Val ROC 0.4998988466623138\n",
            "Epoch 106; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4999978035327448; Val ROC 0.5003941407673007\n",
            "Epoch 107; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4999354756734266; Val ROC 0.5001185982175491\n",
            "Epoch 108; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49957010511525773; Val ROC 0.5000774163745433\n",
            "Epoch 109; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4998978435118919; Val ROC 0.4999790448400257\n",
            "Epoch 110; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4997933523217628; Val ROC 0.5001117878283051\n",
            "Val recall 0.13975803554058075\n",
            "Precision recall 5.032699118601158e-05\n",
            "Epoch 111; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.4995959357018995; Val ROC 0.49980937577838697\n",
            "Epoch 112; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4997002811910555; Val ROC 0.5006042914789633\n",
            "Epoch 113; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4996012767057872; Val ROC 0.5005602091189653\n",
            "Epoch 114; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49968938895239606; Val ROC 0.5003534811591905\n",
            "Epoch 115; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49980196071145194; Val ROC 0.5007319671253244\n",
            "Epoch 116; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49970955111848014; Val ROC 0.5002710991400086\n",
            "Epoch 117; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.49993531894725307; Val ROC 0.500439706594361\n",
            "Epoch 118; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.49985508979646925; Val ROC 0.5003305983553332\n",
            "Epoch 119; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49970660380059184; Val ROC 0.49973195633461265\n",
            "Epoch 120; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4995004837311264; Val ROC 0.5001206073405234\n",
            "Val recall 0.13975803554058075\n",
            "Precision recall 5.032699118601158e-05\n",
            "Epoch 121; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4995910386854685; Val ROC 0.5004111320341863\n",
            "Epoch 122; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.49944400465771094; Val ROC 0.5002887865764754\n",
            "Epoch 123; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.49976571513542006; Val ROC 0.5003196753799238\n",
            "Epoch 124; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.4995827945490988; Val ROC 0.5004430180021372\n",
            "Epoch 125; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49991827059945976; Val ROC 0.500021800389043\n",
            "Epoch 126; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.499771351778327; Val ROC 0.5003547475215384\n",
            "Epoch 127; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.500009203779607; Val ROC 0.5004648848023823\n",
            "Epoch 128; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4999600675744351; Val ROC 0.5002591130466111\n",
            "Epoch 129; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49973916612373936; Val ROC 0.5000684528657846\n",
            "Epoch 130; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49995466736822036; Val ROC 0.5001965154443493\n",
            "Val recall 0.13975803554058075\n",
            "Precision recall 5.032699118601158e-05\n",
            "Epoch 131; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49950531205549537; Val ROC 0.5009254767865551\n",
            "Epoch 132; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4998383722851741; Val ROC 0.49996367717144846\n",
            "Epoch 133; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.5001812758671067; Val ROC 0.5004707338831734\n",
            "Epoch 134; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.49968931001072275; Val ROC 0.5002758558010428\n",
            "Epoch 135; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.499707805972435; Val ROC 0.5002451855272382\n",
            "Epoch 136; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.4998939532565472; Val ROC 0.5003717004121664\n",
            "Epoch 137; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49971662729458366; Val ROC 0.500270829945325\n",
            "Epoch 138; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4998052240996598; Val ROC 0.49994544810641967\n",
            "Epoch 139; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49973573133423577; Val ROC 0.5004933482322831\n",
            "Epoch 140; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49978859527932545; Val ROC 0.500314456360284\n",
            "Val recall 0.13975803554058075\n",
            "Precision recall 5.032699118601158e-05\n",
            "Epoch 141; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4992940800683345; Val ROC 0.5003537303886548\n",
            "Epoch 142; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4997827713982471; Val ROC 0.500291516682568\n",
            "Epoch 143; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49957538298253373; Val ROC 0.49999294785100584\n",
            "Epoch 144; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4999409234665159; Val ROC 0.5004393961541932\n",
            "Epoch 145; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49961403161217255; Val ROC 0.5002156787477272\n",
            "Epoch 146; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4996654257829781; Val ROC 0.5006309177027487\n",
            "Epoch 147; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4996842215851284; Val ROC 0.5000246709208539\n",
            "Epoch 148; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4998435369729683; Val ROC 0.5002543196559923\n",
            "Epoch 149; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.49981098809465296; Val ROC 0.5000084648336208\n",
            "Epoch 150; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49983796903225475; Val ROC 0.5000783541319348\n",
            "Val recall 0.13975803554058075\n",
            "Precision recall 5.032699118601158e-05\n",
            "Epoch 151; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.4995122551961581; Val ROC 0.5005079022589815\n",
            "Epoch 152; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49966504318138494; Val ROC 0.5004338240417578\n",
            "Epoch 153; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49966283029742414; Val ROC 0.4996927659359689\n",
            "Epoch 154; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4998355931947851; Val ROC 0.49997177713101165\n",
            "Epoch 155; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4998143614352116; Val ROC 0.5004442106294209\n",
            "Epoch 156; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4997352620402692; Val ROC 0.4997716152362531\n",
            "Epoch 157; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49980656736275597; Val ROC 0.5003829186891027\n",
            "Epoch 158; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.499628725384047; Val ROC 0.49976898977295336\n",
            "Epoch 159; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.49974559191989154; Val ROC 0.5003817528034574\n",
            "Epoch 160; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49970597179440474; Val ROC 0.5000393476266728\n",
            "Val recall 0.13975803554058075\n",
            "Precision recall 5.032699118601158e-05\n",
            "Epoch 161; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.49964064427159793; Val ROC 0.49987129140304964\n",
            "Epoch 162; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.4997938011341245; Val ROC 0.5002453033944052\n",
            "Epoch 163; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.49960836362885797; Val ROC 0.5000341343885711\n",
            "Epoch 164; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4997732060892578; Val ROC 0.5001999648647522\n",
            "Epoch 165; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4999071461663113; Val ROC 0.4995915497246993\n",
            "Epoch 166; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4996630149549862; Val ROC 0.49993206456071465\n",
            "Epoch 167; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4996420598006891; Val ROC 0.5003554559633002\n",
            "Epoch 168; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4998952500860693; Val ROC 0.5005805789933827\n",
            "Epoch 169; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4996834379484851; Val ROC 0.5002313729844611\n",
            "Epoch 170; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.4994508380329765; Val ROC 0.5006168415598659\n",
            "Val recall 0.13975803554058075\n",
            "Precision recall 5.032699118601158e-05\n",
            "Epoch 171; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49974684330790536; Val ROC 0.5003449358196208\n",
            "Epoch 172; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.49961613039224834; Val ROC 0.5003976854097559\n",
            "Epoch 173; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.49972408815837266; Val ROC 0.5006278272796065\n",
            "Epoch 174; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.499866816538826; Val ROC 0.49965264515320296\n",
            "Epoch 175; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4999608657472606; Val ROC 0.4999648708366979\n",
            "Epoch 176; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.4996586112931589; Val ROC 0.49997244765858084\n",
            "Epoch 177; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49949392119472824; Val ROC 0.4998073539895478\n",
            "Epoch 178; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.4998749229381411; Val ROC 0.500091420546034\n",
            "Epoch 179; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49973481777915946; Val ROC 0.49989795353980226\n",
            "Epoch 180; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4998921311922283; Val ROC 0.49952301661581644\n",
            "Val recall 0.13975803554058075\n",
            "Precision recall 5.032699118601158e-05\n",
            "Epoch 181; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49961332040621276; Val ROC 0.49989145375792354\n",
            "Epoch 182; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.49980564398488975; Val ROC 0.4996093963852825\n",
            "Epoch 183; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.4997833899335303; Val ROC 0.4997588436402715\n",
            "Epoch 184; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.499726182355575; Val ROC 0.5000855391618336\n",
            "Epoch 185; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4996856749776527; Val ROC 0.5001229582774123\n",
            "Epoch 186; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49966722225222804; Val ROC 0.5003334836692567\n",
            "Epoch 187; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49956657478110084; Val ROC 0.4997744003529261\n",
            "Epoch 188; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49978067401185755; Val ROC 0.5004411547536061\n",
            "Epoch 189; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4996043440235835; Val ROC 0.5001657207285434\n",
            "Epoch 190; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4997567659887074; Val ROC 0.49996307404008206\n",
            "Val recall 0.13975803554058075\n",
            "Precision recall 5.032699118601158e-05\n",
            "Epoch 191; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.4998516343400865; Val ROC 0.5007023020750754\n",
            "Epoch 192; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49963508286555824; Val ROC 0.5006884293801136\n",
            "Epoch 193; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4996194677371833; Val ROC 0.4994888279004728\n",
            "Epoch 194; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49983451192879647; Val ROC 0.5009101310978615\n",
            "Epoch 195; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49961874396107103; Val ROC 0.5003153479282189\n",
            "Epoch 196; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.4998872509428195; Val ROC 0.5001159664849549\n",
            "Epoch 197; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.49985304922536394; Val ROC 0.500207346819122\n",
            "Epoch 198; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49958386133591975; Val ROC 0.5001806420510865\n",
            "Epoch 199; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4997869927410267; Val ROC 0.4999612414036227\n",
            "Epoch 200; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.49984809204300024; Val ROC 0.5000543634159176\n",
            "Val recall 0.13975803554058075\n",
            "Precision recall 5.032699118601158e-05\n",
            "Epoch 201; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49969060780753005; Val ROC 0.49986860665121896\n",
            "Epoch 202; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49988191030338175; Val ROC 0.49993041728051235\n",
            "Epoch 203; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.49964521873495116; Val ROC 0.5001446766944686\n",
            "Epoch 204; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.5000001051206368; Val ROC 0.5001522770757137\n",
            "Epoch 205; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49974952793211674; Val ROC 0.49995842269813884\n",
            "Epoch 206; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.4999173595662351; Val ROC 0.4996252495307116\n",
            "Epoch 207; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.49977589987082777; Val ROC 0.4998909430303715\n",
            "Epoch 208; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49977252980560855; Val ROC 0.5004041301711726\n",
            "Epoch 209; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.49970146404853344; Val ROC 0.5007980034703062\n",
            "Epoch 210; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.4994921489608694; Val ROC 0.5001523825402624\n",
            "Val recall 0.13975803554058075\n",
            "Precision recall 5.032699118601158e-05\n",
            "Epoch 211; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.4997568787161035; Val ROC 0.5001531457008996\n",
            "Epoch 212; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.49958450588260706; Val ROC 0.5000806985133577\n",
            "Epoch 213; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.499770572506178; Val ROC 0.4998780300509451\n",
            "Epoch 214; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49978227113177487; Val ROC 0.5002947454005566\n",
            "Epoch 215; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.4997735789332389; Val ROC 0.49995907520756305\n",
            "Epoch 216; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49978610364030157; Val ROC 0.5000362554987535\n",
            "Epoch 217; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.499871017752318; Val ROC 0.5003672711880812\n",
            "Epoch 218; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49980653586717916; Val ROC 0.5003339474788793\n",
            "Epoch 219; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49964429077530764; Val ROC 0.5001351104535607\n",
            "Epoch 220; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.4997564615294499; Val ROC 0.5003240435134658\n",
            "Val recall 0.13975803554058075\n",
            "Precision recall 5.032699118601158e-05\n",
            "Epoch 221; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4999822274333009; Val ROC 0.4998045763690714\n",
            "Epoch 222; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4996882521419716; Val ROC 0.500378808471759\n",
            "Epoch 223; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.5000305858391926; Val ROC 0.5000685721088682\n",
            "Epoch 224; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.5001604895099403; Val ROC 0.5003355223244395\n",
            "Epoch 225; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.5000497827858738; Val ROC 0.5002997444031708\n",
            "Epoch 226; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49989734720632506; Val ROC 0.5002720832659112\n",
            "Epoch 227; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49954926625073814; Val ROC 0.49980967920497704\n",
            "Epoch 228; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.5001024368875361; Val ROC 0.5004357764379119\n",
            "Epoch 229; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.4996959317207629; Val ROC 0.5002451420236059\n",
            "Epoch 230; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4996021329834271; Val ROC 0.5001635164475362\n",
            "Val recall 0.13975803554058075\n",
            "Precision recall 5.032699118601158e-05\n",
            "Epoch 231; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.4999320235046437; Val ROC 0.4999435130363019\n",
            "Epoch 232; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49967646944126387; Val ROC 0.4999980429444647\n",
            "Epoch 233; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4998646168405336; Val ROC 0.5001208787494804\n",
            "Epoch 234; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4997399332676578; Val ROC 0.5002554859918442\n",
            "Epoch 235; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.49982364213864267; Val ROC 0.5000395461689627\n",
            "Epoch 236; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.5000830189056665; Val ROC 0.4997753699840738\n",
            "Epoch 237; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4997226455455018; Val ROC 0.500805034031124\n",
            "Epoch 238; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49951661784343987; Val ROC 0.49982033968109263\n",
            "Epoch 239; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.49938003338419684; Val ROC 0.5001690064512232\n",
            "Epoch 240; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4999609415195901; Val ROC 0.49983654989607634\n",
            "Val recall 0.13975803554058075\n",
            "Precision recall 5.032699118601158e-05\n",
            "Epoch 241; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4995948138590921; Val ROC 0.5001657002028383\n",
            "Epoch 242; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4997078579647828; Val ROC 0.5004885845383024\n",
            "Epoch 243; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49978474725065014; Val ROC 0.5002322124458256\n",
            "Epoch 244; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49966246856557; Val ROC 0.5004269351106351\n",
            "Epoch 245; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4997026292630186; Val ROC 0.499872421924355\n",
            "Epoch 246; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49967100708737366; Val ROC 0.5004035263054042\n",
            "Epoch 247; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.4996157737022906; Val ROC 0.5006132945414737\n",
            "Epoch 248; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4997729715180843; Val ROC 0.5000005517698707\n",
            "Epoch 249; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.49989260527502594; Val ROC 0.5005089784886929\n",
            "Epoch 250; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4996832335132436; Val ROC 0.5001647373603632\n",
            "Val recall 0.13975803554058075\n",
            "Precision recall 5.032699118601158e-05\n",
            "Epoch 251; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49979845768677944; Val ROC 0.49935825339565015\n",
            "Epoch 252; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4995311872796936; Val ROC 0.5000569275337277\n",
            "Epoch 253; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49969383807828627; Val ROC 0.49983619441192895\n",
            "Epoch 254; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.4997211577603218; Val ROC 0.5004544567150793\n",
            "Epoch 255; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.4999107816035842; Val ROC 0.5000080212485112\n",
            "Epoch 256; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4996025299744535; Val ROC 0.5005365657470124\n",
            "Epoch 257; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4998783032693309; Val ROC 0.5003344237598961\n",
            "Epoch 258; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49996191745717783; Val ROC 0.5005576773919181\n",
            "Epoch 259; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49968478811460904; Val ROC 0.5001851805144991\n",
            "Epoch 260; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4997423916490959; Val ROC 0.4997457449864005\n",
            "Val recall 0.13975803554058075\n",
            "Precision recall 5.032699118601158e-05\n",
            "Epoch 261; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49983370188448545; Val ROC 0.4998716332221028\n",
            "Epoch 262; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49967461804777236; Val ROC 0.5002754576796837\n",
            "Epoch 263; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.4996874490307277; Val ROC 0.5004295792296414\n",
            "Epoch 264; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4996099527535263; Val ROC 0.4997842111105561\n",
            "Epoch 265; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49983344313858097; Val ROC 0.5002941418790638\n",
            "Epoch 266; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.5001013505754238; Val ROC 0.49976209984813486\n",
            "Epoch 267; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4994240202951742; Val ROC 0.49999237005808816\n",
            "Epoch 268; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.49983676945189454; Val ROC 0.5000828932862703\n",
            "Epoch 269; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.4997193880357953; Val ROC 0.5004905084240081\n",
            "Epoch 270; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4996502236633926; Val ROC 0.49980523493474255\n",
            "Val recall 0.13975803554058075\n",
            "Precision recall 5.032699118601158e-05\n",
            "Epoch 271; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49978073787982197; Val ROC 0.5002891282967125\n",
            "Epoch 272; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49967720823271605; Val ROC 0.5001788402227499\n",
            "Epoch 273; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4998752068665222; Val ROC 0.49989673546220964\n",
            "Epoch 274; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.49981867155803894; Val ROC 0.4998957184973135\n",
            "Epoch 275; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4996614069103138; Val ROC 0.4997288418540744\n",
            "Epoch 276; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4997098379713992; Val ROC 0.5002623809705179\n",
            "Epoch 277; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49997550987820755; Val ROC 0.5003066620174728\n",
            "Epoch 278; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.49979321590876374; Val ROC 0.4999666392667414\n",
            "Epoch 279; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.4998085545622184; Val ROC 0.5001419789937639\n",
            "Epoch 280; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49960049054945405; Val ROC 0.5000502538068865\n",
            "Val recall 0.13975803554058075\n",
            "Precision recall 5.032699118601158e-05\n",
            "Epoch 281; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49961732940297543; Val ROC 0.5001057374261175\n",
            "Epoch 282; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4998366275816643; Val ROC 0.500389458291932\n",
            "Epoch 283; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4998816414153461; Val ROC 0.4996923838923998\n",
            "Epoch 284; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49990345788675994; Val ROC 0.49981247969191617\n",
            "Epoch 285; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.5000540452495819; Val ROC 0.4999431461037056\n",
            "Epoch 286; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4994515949146995; Val ROC 0.5001412577118255\n",
            "Epoch 287; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.49993799566456176; Val ROC 0.5001727528826734\n",
            "Epoch 288; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4996709155368606; Val ROC 0.5001949619458543\n",
            "Epoch 289; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4997550634766958; Val ROC 0.499883023047116\n",
            "Epoch 290; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4998243914452911; Val ROC 0.4999470041749262\n",
            "Val recall 0.13975803554058075\n",
            "Precision recall 5.032699118601158e-05\n",
            "Epoch 291; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4997517645259771; Val ROC 0.5004070652631615\n",
            "Epoch 292; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4997304689976594; Val ROC 0.49978829201585423\n",
            "Epoch 293; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.49969128855463496; Val ROC 0.5001437424478193\n",
            "Epoch 294; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.49981797119200555; Val ROC 0.500253773647106\n",
            "Epoch 295; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4997109461098546; Val ROC 0.5002252402533629\n",
            "Epoch 296; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.499834590151153; Val ROC 0.5005507747933299\n",
            "Epoch 297; Train loss 0.6931471824645996; Val loss 0.6931472420692444; Train ROC 0.4996301485354825; Val ROC 0.5005992298454586\n",
            "Epoch 298; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4997161018273386; Val ROC 0.5002999658052041\n",
            "Epoch 299; Train loss 0.6931473016738892; Val loss 0.6931472420692444; Train ROC 0.4996429726842028; Val ROC 0.4995868940648841\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(tensor(0.6931),\n",
              " np.float64(0.5002370256602939),\n",
              " tensor([[ 46588, 139090, 134460,  ..., 169835,  10761, 176501],\n",
              "         [215321, 190636, 203650,  ..., 233667, 249381, 285154]]),\n",
              " tensor([0., 0., 0.,  ..., 0., 0., 0.]))"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model2 = GCN(\n",
        "    num_nodes = num_nodes, num_layers = args['num_layers'],\n",
        "    embedding_dim = args[\"emb_size\"], conv_layer = \"LGC\"\n",
        ")\n",
        "train(datasets, model2, optimizer, args)\n",
        "test(model2, datasets['test'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f3b848e-f75c-43e8-aa81-44bdae7d7b1c",
      "metadata": {
        "id": "5f3b848e-f75c-43e8-aa81-44bdae7d7b1c",
        "outputId": "8bb40aba-a480-4c1e-8cc6-716d5bed217e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current working directory: D:\\CPEN355\n",
            "File exists: True\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Get the current working directory\n",
        "current_dir = os.getcwd()\n",
        "print(\"Current working directory:\", current_dir)\n",
        "\n",
        "file_path = r\"D:\\CPEN355\\model_stats\\LGC_RUN5.pkl\"\n",
        "print(\"File exists:\", os.path.exists(file_path))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "484b6f54-f511-4d6d-b552-614458acada0",
      "metadata": {
        "id": "484b6f54-f511-4d6d-b552-614458acada0"
      },
      "outputs": [],
      "source": [
        "\n",
        "lgc_stats = pickle.load(open(r\"D:\\CPEN355\\model_stats\\LGC_RUN5.pkl\", \"rb\"))\n",
        "lgc_stats_hard = pickle.load(open(r\"D:\\CPEN355\\model_stats\\LGC_RUN6.pkl\", \"rb\"))\n",
        "gat_stats = pickle.load(open(r\"D:\\CPEN355\\model_stats\\GAT_RUN5.pkl\", \"rb\"))\n",
        "gat_stats_hard = pickle.load(open(r\"D:\\CPEN355\\model_stats\\GAT_RUN6.pkl\", \"rb\"))\n",
        "sage_stats = pickle.load(open(r\"D:\\CPEN355\\model_stats\\SAGE_RUN5.pkl\", \"rb\"))\n",
        "sage_stats_hard = pickle.load(open(r\"D:\\CPEN355\\model_stats\\SAGE_RUN6.pkl\", \"rb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a51c3585-f391-42b5-b996-b519e9fc6981",
      "metadata": {
        "id": "a51c3585-f391-42b5-b996-b519e9fc6981"
      },
      "outputs": [],
      "source": [
        "def detach_loss(stats):\n",
        "  return [loss.detach().cpu().numpy().item() for loss in stats]\n",
        "\n",
        "def plot_train_val_loss(stats_dict):\n",
        "  fig, ax = plt.subplots(1,1, figsize = (6, 4))\n",
        "  train_loss = detach_loss(stats_dict[\"train\"][\"loss\"])\n",
        "  val_loss = detach_loss(stats_dict[\"val\"][\"loss\"])\n",
        "  idx = np.arange(0, len(train_loss), 1)\n",
        "  ax.plot(idx, train_loss, label = \"train\")\n",
        "  ax.plot(idx, val_loss, label = \"val\")\n",
        "  ax.legend()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6733596f-bac0-49eb-ad39-c7c81ecfd518",
      "metadata": {
        "id": "6733596f-bac0-49eb-ad39-c7c81ecfd518",
        "outputId": "5bf4207c-ee8d-4e6c-b4e7-636be73b01d5"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plot_train_val_loss(lgc_stats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df95f647-0cde-4018-aa94-d21ecd150f5d",
      "metadata": {
        "id": "df95f647-0cde-4018-aa94-d21ecd150f5d"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1,1, figsize = (8, 6))\n",
        "key = \"loss\"\n",
        "lgc_loss = pd.Series(detach_loss(lgc_stats[\"val\"][key])).rolling(3).mean()\n",
        "gat_loss = pd.Series(detach_loss(gat_stats[\"val\"][key])).rolling(3).mean()\n",
        "sage_loss = pd.Series(detach_loss(sage_stats[\"val\"][key])).rolling(3).mean()\n",
        "lgc_hard_loss = pd.Series(detach_loss(lgc_stats_hard[\"val\"][key])).rolling(3).mean()\n",
        "gat_hard_loss = pd.Series(detach_loss(gat_stats_hard[\"val\"][key])).rolling(3).mean()\n",
        "sage_hard_loss = pd.Series(detach_loss(sage_stats_hard[\"val\"][key])).rolling(3).mean()\n",
        "idx = np.arange(0, len(lgc_loss), 1)\n",
        "\n",
        "colors = [\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"]\n",
        "ax.plot(idx, lgc_loss, color = colors[0], linestyle = 'dashed', label = \"LGC - random\")\n",
        "ax.plot(idx, lgc_hard_loss, color = colors[0], label = \"LGC - hard\")\n",
        "ax.plot(idx, gat_loss, color = colors[1], linestyle = 'dashed', label = \"GAT - random\")\n",
        "ax.plot(idx, gat_hard_loss, color = colors[1], label = \"GAT - hard\")\n",
        "ax.plot(idx, sage_loss, color = colors[2], linestyle = 'dashed', label = \"SAGE - random\")\n",
        "ax.plot(idx, sage_hard_loss, color = colors[2], label = \"SAGE - hard\")\n",
        "ax.legend(loc = 'lower left')\n",
        "\n",
        "ax.set_xlabel(\"Epochs\")\n",
        "ax.set_ylabel(\"BPR Loss\")\n",
        "ax.set_title(\"Model BPR Loss, by convolution type and negative sampling\")\n",
        "ax.set_ylim(0, 0.7)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ec1ee0a-5cf3-4880-8275-9094548cade1",
      "metadata": {
        "id": "6ec1ee0a-5cf3-4880-8275-9094548cade1"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1,1, figsize = (8, 6))\n",
        "key = \"recall\"\n",
        "lgc_recall = lgc_stats[\"val\"][key]\n",
        "gat_recall = gat_stats[\"val\"][key]\n",
        "sage_recall = sage_stats[\"val\"][key]\n",
        "lgc_hard_recall = lgc_stats_hard[\"val\"][key]\n",
        "gat_hard_recall = gat_stats_hard[\"val\"][key]\n",
        "sage_hard_recall = sage_stats_hard[\"val\"][key]\n",
        "# increment by 10\n",
        "idx = np.arange(0, 10 * len(lgc_recall), 10)\n",
        "\n",
        "colors = [\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"]\n",
        "ax.plot(idx, lgc_recall, color = colors[0], linestyle = 'dashed', label = \"LGC - random\")\n",
        "ax.plot(idx, lgc_hard_recall, color = colors[0], label = \"LGC - hard\")\n",
        "ax.plot(idx, gat_recall, color = colors[1], linestyle = 'dashed', label = \"GAT - random\")\n",
        "ax.plot(idx, gat_hard_recall, color = colors[1], label = \"GAT - hard\")\n",
        "ax.plot(idx, sage_recall, color = colors[2], linestyle = 'dashed', label = \"SAGE - random\")\n",
        "ax.plot(idx, sage_hard_recall, color = colors[2], label = \"SAGE - hard\")\n",
        "ax.legend(loc = 'lower right')\n",
        "\n",
        "ax.set_xlabel(\"Epochs\")\n",
        "ax.set_ylabel(\"Recall@300\")\n",
        "ax.set_title(\"Model Recall@300, by convolution type and negative sampling\")\n",
        "ax.set_ylim(0, 0.7)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "myenv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}